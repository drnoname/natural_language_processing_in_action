{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "在Chatbot 中，需要自动生成text。\n",
    "\n",
    "通常的流程是：\n",
    "1. 给定一个set of parameters，\n",
    "2. 生成一个set of text\n",
    "3. 选出得分最高的candidate\n",
    "\n",
    "下面我们看，如何使用LSTM 生成text。\n",
    "\n",
    "通常，基于概率模型的文本生成方法，例如马尔可夫方法，是计算一个条件概率分布，即基于前面几个单词(n-gram)， 预测下一个单词的概率。RNN 和LSTM 的方法类似，区别在于：\n",
    "- RNN encode information -- feature extraction\n",
    "- LSTM: memory state has greater context -- better performance\n",
    "\n",
    "下面我们介绍，如何使用LSTM 自动生成text.\n",
    "\n",
    "如果要做prediction，我们需要修改network 的结构。这不是一个sentiment analysis 的分类问题了。这个word embedding 类似，是一个self-supervised learning. \n",
    "\n",
    "<img src=\"img/next_word_prediction.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "这里，我们不使用之前的IMDB 数据库，原因是：\n",
    "- 数据库小\n",
    "- 异质化比较严重。异质化的意思是，review 是不同人写的，大家有不同的书写风格。\n",
    "\n",
    "所以我们下面通过学习莎士比亚的文章，来生成莎士比亚风格的文本(singular style)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取所有莎士比亚的作品，并把他们拼接成一个large string。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 375542\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for txt in gutenberg.fileids():\n",
    "    if 'shakespeare' in txt:\n",
    "        text += gutenberg.raw(txt).lower()\n",
    "\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of julius caesar by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter flauius, murellus, and certaine commoners ouer the stage.\n",
      "\n",
      "  flauius. hence: home you idle creatures, get you home:\n",
      "is this a holiday? what, know you not\n",
      "(being mechanicall) you ought not walke\n",
      "vpon a labouring day, without the signe\n",
      "of your profession? speake, what trade art thou?\n",
      "  car. why sir, a carpenter\n",
      "\n",
      "   mur. where is thy leather apron, and thy rule?\n",
      "what dost thou with thy best apparrell on\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们统计所有出现过的characters，类似于构建字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 50\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们构建一个training set. 构建方法：对于刚才构建的text string，我们选取大小为40的滑动窗口，step = 3，构建训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 125168\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[the tragedie of julius caesar by willia', 'm')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0], next_chars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构建了125,168 个这样的sequences 作为训练集。下面，我们对每个sequence 做one-hot 编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们构建model.\n",
    "\n",
    "- 我们不需要每个步骤的输出，只需要最后一个输出，所以不需要`return_sequences=True` 参数\n",
    "- 因为这个问题更复杂，所以我们使用了更大的network，LSTM 有128 个neurons\n",
    "- 使用RMSprop 作为优化器\n",
    "- loss function\n",
    "- no dropout: 我们需要生成text 和莎士比亚越像越好，所以追求过拟合。所以这和传统的方法追求泛化是不同的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' # 避免notebook 执行时退出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "\n",
    "with open(\"shakes_lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 258s 2ms/step - loss: 2.0654\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 256s 2ms/step - loss: 1.6970\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 254s 2ms/step - loss: 1.5918\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 253s 2ms/step - loss: 1.5327\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 254s 2ms/step - loss: 1.4875\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 252s 2ms/step - loss: 1.4578\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 255s 2ms/step - loss: 1.4337\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 252s 2ms/step - loss: 1.4147\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 251s 2ms/step - loss: 1.3993\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 251s 2ms/step - loss: 1.3846\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 251s 2ms/step - loss: 1.3704\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 251s 2ms/step - loss: 1.3614\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 251s 2ms/step - loss: 1.3526\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 252s 2ms/step - loss: 1.3430\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 251s 2ms/step - loss: 1.3337\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 251s 2ms/step - loss: 1.3275\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 254s 2ms/step - loss: 1.3201\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 253s 2ms/step - loss: 1.3147\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 253s 2ms/step - loss: 1.3100\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 255s 2ms/step - loss: 1.3011\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 253s 2ms/step - loss: 1.2990\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 253s 2ms/step - loss: 1.2954\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 252s 2ms/step - loss: 1.2910\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 254s 2ms/step - loss: 1.2856\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 253s 2ms/step - loss: 1.2784\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 265s 2ms/step - loss: 1.2760\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 257s 2ms/step - loss: 1.2759\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 255s 2ms/step - loss: 1.2683\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 278s 2ms/step - loss: 1.2662\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 279s 2ms/step - loss: 1.2624\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit(X, y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "\n",
    "    model.save_weights(\"shakes_lstm_weights_{}.h5\".format(i+1))\n",
    "    print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次我们训练6个epoch，然后保存一下模型参数。一共训练5轮，即30个epoch. \n",
    "\n",
    "character level 的好处是不用去考虑tokenization 和sentence segmentation. 但是要注意，case-folding 是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate text\n",
    "\n",
    "下面，我们输出text 生成器。\n",
    "\n",
    "⚠️ 我们不是选取概率最高的那个character，而是按照概率分布随机选取一个。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `temperature` (后面叫做diversity) 的作用\n",
    "\n",
    "- 当temperature 小于1 时：\n",
    "    - sharpening the probability distribution\n",
    "    - more strict attempt to recreate the original text -- 更像莎士比亚\n",
    "- 当temperature 大于1 时：\n",
    "    - flattening the probability distribution\n",
    "    - more diverse text -- 更不像莎士比亚\n",
    "\n",
    "下面，我们用训练好的模型来生成一段话。\n",
    "\n",
    "- numpy random function `multinomial` 返回num_samples from the distribution described by `probabilities_list`. 这里，我们只需要输出一个output.\n",
    "- 这里和training 不同，我们首先选一个长度为40的时间窗，然后每次预测一个character，然后往前移动一个step，继续预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"that well might\n",
      "aduise him to a caution,\"\n",
      "that well might\n",
      "aduise him to a caution, and the more and strong,\n",
      "the strong to the things the strong to the reason\n",
      "the strong to the true blood, and with the treale,\n",
      "and the strong to the heard, and with the strent: but that he worlowes\n",
      "that i will strong the things and the treate\n",
      "the straige them sir, and the strong to the heard,\n",
      "and the strong to the things and the strent.\n",
      "the strong to the things in the capitors,\n",
      "the presvtxxxvxxxip\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"that well might\n",
      "aduise him to a caution,\"\n",
      "that well might\n",
      "aduise him to a caution, and my lord\n",
      "\n",
      "   macb. he was he dengerost him of my moutio,\n",
      "and then he did hamlet them. the cause, and that\n",
      "so farewers, i shall sir, why shall be breath,\n",
      "it is not are them. and the great feare,\n",
      "the strong to the straine the recordumes to the man,\n",
      "and that we you mourne he may, i will not\n",
      "the stole to the had strong to do my noble to me,\n",
      "that not loue he you words then i haue the capt:\n",
      "a too fu\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"that well might\n",
      "aduise him to a caution,\"\n",
      "that well might\n",
      "aduise him to a caution, and what shew youtless:\n",
      "for and the lyua't we that haue all rome,\n",
      "and looking 'tweeare are minst werch you;\n",
      "that's your take it so?\n",
      "  kin. ye was wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenwang/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uld predgall\n",
      "hath start the play the will knocking puts ale\n",
      "that were lucius i lyfe at wake the caesar\n",
      "is not tongue the reuellory thee\n",
      "\n",
      "   ophe. and thou laue. some starn'd th' eorion you sweet backe\n",
      "of my wromany dickne louds,\n",
      "and to him bach childr\n",
      "\n",
      "----- diversity: 1.5\n",
      "----- Generating with seed: \"that well might\n",
      "aduise him to a caution,\"\n",
      "that well might\n",
      "aduise him to a caution, :wand: and your vi'streast\n",
      "broates, so wilth\n",
      "\n",
      "side chrimsany.\n",
      "\n",
      "llare all your forc'd.\n",
      "touke from slaught?\n",
      "hold out, prinatarriusepp't: and murther,\n",
      "enter.\n",
      "\n",
      "alalm\n",
      "\n",
      "   'tis that -  rounded nem. by thoses abour\n",
      "\n",
      " exit; you well,\n",
      "thosgh i sdownepsarmies, cey withing cracresse haed\n",
      "buthchnemess saceing from her shoo: houre:\n",
      "lad\n",
      "in dye same butsure, yay, poblowed all noif'd,\n",
      "and eueralsa wife? trepon t\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "random.seed(42)\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "for diversity in [0.2, 0.5, 1.0, 1.5]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ 上述出现了以下问题：\n",
    "\n",
    "/Users/chenwang/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
    "\n",
    "to check..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make it more useful\n",
    "\n",
    "上面我们讲的这个example 就是have fun，下面我们来看，如果想在真实场景中使用generative model，我们应该怎样做。\n",
    "\n",
    "- Expand the quantity and quality of the corpus.\n",
    "- Expand the complexity of the model (number of neurons).\n",
    "- Implement a more refined case folding algorithm.\n",
    "- Segment sentences.\n",
    "- Add filters on grammar, spelling, and tone to match your needs.\n",
    "- Generate many more examples than you actually show your users.\n",
    "- Use seed texts chosen from the context of the session to steer the chatbot toward useful topics.\n",
    "- Use multiple different seed texts within each dialog round to explore what the chatbot can talk about well and what the user finds helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. how to say, and what to say\n",
    "\n",
    "现在我们已经演示了how to say，but you have no control on what is being said. 也就是说，可能会答非所问。\n",
    "\n",
    "- 可以尝试使用一个不存在的词来开始一句话，来看看interesting results。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extensions \n",
    "\n",
    "### 3.1 Other kinds of memory\n",
    "\n",
    "其他的memory 在gate 的operation 会有稍许区别。\n",
    "\n",
    "#### GRU\n",
    "\n",
    "更高效：更少的参数\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(num_neurons, return_sequences=True, input_shape=X[0].shape))\n",
    "```\n",
    "\n",
    "#### peephole connections\n",
    "\n",
    "`Learning Precise Timing with LSTM Recurrent Networks`\n",
    "\n",
    "区别在于，input 现在变成三个信号的叠加：\n",
    "- input at time t\n",
    "- output at time t-1\n",
    "- memory state\n",
    "\n",
    "对于time series data 比较有效\n",
    "\n",
    "### 3.2 Going deeper\n",
    "\n",
    "叠加(stack)多个LSTM 层, 注意，第一层和中间层的`return_sequences=True`.\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape))\n",
    "model.add(LSTM(num_neurons_2, return_sequences=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
