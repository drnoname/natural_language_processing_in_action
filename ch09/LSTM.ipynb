{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "RNN 的一个问题是当sequence 特别长的时候，因为loss 在向回传递逐层减弱，所以有时会无法扑捉到token 之间的关系，考虑下面两句话：\n",
    "\n",
    "- The young woman went to the movies with her friends.\n",
    "\n",
    "- The young woman, having found a free ticket on the ground, went to the movies.\n",
    "\n",
    "第一句话因为主语和谓语紧挨着，所以很容易捕捉这类关系。第二句话，主语和谓语之间加入了一个从句，所以很有可能无法捕捉主语和谓语的关系。\n",
    "\n",
    "⚠️ 没有捕捉到这个关系的影响是什么？\n",
    "\n",
    "LSTM 解决这个问题的方法是加入了一个`state` 的concept， 这个state 可以看作是memory。memory 的作用是，通过training，可以学习到what to remember, 同时，网络其余的部分学习如何利用remember的和输入的数据来做预测。\n",
    "\n",
    "通过memory，可以捕获到更长的依赖关系。\n",
    "\n",
    "⚠️ hard to think.\n",
    "\n",
    "使用LSTM，除了可以predict，还可以generate text.\n",
    "\n",
    "LSTM 网络图如下：\n",
    "\n",
    "<img src=\"img/lstm.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "可以看出，这是一个RNN unrolled version + memory state.\n",
    "\n",
    "下面，我们来看每个LSTM Layer。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layer\n",
    "\n",
    "#### 输入\n",
    "\n",
    "- input instance of current time step\n",
    "    - 300-element vector\n",
    "- output from previous time step\n",
    "    - 50-element vector\n",
    "- concatenation: 把两个input vectors 拼接成一个长度为350-element vector.\n",
    "\n",
    "<img src=\"img/lstm_layer_input.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "input 一共经过3个gates，每个gate 都是一个feed forward network layer, gate 的weights 决定了有多少信息可以go through to the cell's state (memory).\n",
    "\n",
    "- forget gate\n",
    "- input/candidate gate (2 branches)\n",
    "- update/output gate\n",
    "\n",
    "参数个数分析：\n",
    "- 每个gate 的每个neuron 连接为长度为350 的vector + 1个bias， 总共351 个weights。\n",
    "- 每个gate有50个neurons，总共为351 * 50 = 17550\n",
    "- 一共3个gates, candidate gate 有两个分支，参数个数一样，总共可以看作4个gates，参数个数为：17750 * 4 = 70200\n",
    "- output layer，LSTM 的输出是400 * 50 (每个step 输出长度为50的\"thought vector\"，一共50 steps), flatten 之后长度20000，加一个bias 一共20001\n",
    "- 总共 70200 + 20001 = 90201 个参数\n",
    "\n",
    "<img src=\"img/forget_gate.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "注意，对于第一个token，step t-1 的50-element vector 补零。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Forget Gate\n",
    "\n",
    "the goal is to learn how much of the cell's memory you want to erase. The idea behind wanting to forget is as important as wanting to remember.\n",
    "\n",
    "forget gate 本身是一个feed forward network:\n",
    "- n neurons\n",
    "- m + n + 1 weights for each neuron (300 + 50 + 1)\n",
    "- activation function: sigmoid\n",
    "- output: 0 ~ 1\n",
    "\n",
    "<img src=\"img/forget_gate_weights.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "forget gate 的输出类似于一个mask, 值接近1 代表通过率高，即保留记忆；值接近0 代表通过率低，即删除记忆。然后这个\"mask\" 和memory vector 做element-wise 乘法，更新memory，过程如下图所示。这就是forget gate 怎么做到forget things 的。forget 是指，更新memory，使某些维度的信息量减少。\n",
    "\n",
    "<img src=\"img/forget_gate_calculation.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Candidate gate\n",
    "\n",
    "goal: how much to augment the memory based on:\n",
    "- concatenated input\n",
    "    - input of step t\n",
    "    - output of step t-1\n",
    "    \n",
    "如下图所示，candidate gate 包含 2 个 branches:\n",
    "1. decide which input vector elements are worth remembering\n",
    "    - 类似于forget gate, sigmoid function, 输出 0 ～ 1\n",
    "2. Rout the remembered input elements to the right memory slot.\n",
    "    - what value you are going to update the memory with?\n",
    "    - activation: tanh\n",
    "    - -1 ~ 1\n",
    "\n",
    "<img src=\"img/candidate_gate.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Output:\n",
    "- 然后我们把两个vector 做element-wise multiplication. \n",
    "\n",
    "最后，这个output 和之前的updated memory 做element-wise addition，实现remember new things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Output/Update gate\n",
    "\n",
    "flow 1 (gate): \n",
    "- Input_1: concatenated input\n",
    "- n neurons\n",
    "- activation function: sigmoid\n",
    "- output_1: n-dimensional output between 0 and 1\n",
    "\n",
    "flow 2 (mask):\n",
    "- input_2: updated memory vector\n",
    "- tanh function applied elementwise\n",
    "- output_2: n-dimensional vector （value between -1 and 1）\n",
    "\n",
    "注意，这里直接使用tanh function，并没有neuron（即没有weight），所以可以称为mask，但不能称为gate。\n",
    "\n",
    "然后，output_1 element-wise multiplication with output_2.\n",
    "- 生成一个新的n-dimensional vector (step official output)\n",
    "    - 传到step t+1\n",
    "    - layer's output\n",
    "\n",
    "整个过程如下图所示。\n",
    "\n",
    "<img src=\"img/update_gate.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "⚠️ 图上又个bug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM for sentimantal analysis\n",
    "\n",
    "### 1. Load and preprocess the IMDB data\n",
    "\n",
    "前面预处理的流程和RNN都差不多，所以我们把预处理函数下载一个utils.py 的文件中，然后调用他们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import pre_process_data, tokenize_and_vectorize, collect_expected, pad_trunc\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' # 避免notebook 执行时退出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_datasets = '/Users/chenwang/Workspace/datasets/IMDB/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pre_process_data(imdb_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data)*.8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "epochs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))  # 20000 * 400 * 300\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a keras LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 400, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 20001     \n",
      "=================================================================\n",
      "Total params: 90,201\n",
      "Trainable params: 90,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_neurons = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit your LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 1000s 50ms/step - loss: 0.4689 - acc: 0.7803 - val_loss: 0.3822 - val_acc: 0.8382\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 969s 48ms/step - loss: 0.3463 - acc: 0.8544 - val_loss: 0.3858 - val_acc: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a304f7550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果中可以看出，validation accuracy 大大提升，同时，训练时间也大大提升。\n",
    "\n",
    "The beauty of the algorithm is that it learns the relationships of the tokens it sees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prediction\n",
    "\n",
    "我们使用了一些positive 的词，在一句表达negative 的观点的评论中，来看一下模型的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample's sentiment, 1 - pos, 0 - neg : [[0]]\n",
      "Raw output of sigmoid function: [[0.19533193]]\n"
     ]
    }
   ],
   "source": [
    "sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\"\n",
    "\n",
    "# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "\n",
    "# Tokenize returns a list of the data (length 1 here)\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "\n",
    "print(\"Sample's sentiment, 1 - pos, 0 - neg : {}\".format(model.predict_classes(test_vec)))\n",
    "print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error analysis\n",
    "\n",
    "try to think statistically:\n",
    "- Are the words in the misclassified example rare? \n",
    "- Are they rare in your corpus or the corpus that trained the language model for your embedding? \n",
    "- Do all of the words in the example exist in your model’s vocabulary?\n",
    "\n",
    "Going through this process of examining the probabilities and input data associated\n",
    "with incorrect predictions helps build your machine learning intuition so you\n",
    "can build better NLP pipelines in the future. This is backpropagation through the\n",
    "human brain for the problem of model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save and reload models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"lstm_weights1.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import model_from_json\n",
    "# with open(\"lstm_model1.json\", \"r\") as json_file:\n",
    "#     json_string = json_file.read()\n",
    "# model = model_from_json(json_string)\n",
    "\n",
    "# model.load_weights('lstm_weights1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dirty Data\n",
    "\n",
    "在NLP pipeline 中，我们有一些dirty data 需要处理。\n",
    "\n",
    "### 2.1 Padding\n",
    "\n",
    "dirty data 是指我们加入的padding, 其实是破坏了数据的integrity (完整性).\n",
    "\n",
    "因为我们是做分类，所以最后需要一个fix length vector (thought vector)，这是我们做padding 的原因。\n",
    "\n",
    "**但是，400 是否是padding 的最佳值呢？**\n",
    "\n",
    "下面我们来看看，数据集的平均长度是多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_len(data, maxlen):\n",
    "    total_len = truncated = exact = padded = 0\n",
    "    for sample in data:\n",
    "        total_len += len(sample)\n",
    "        if len(sample) > maxlen:\n",
    "            truncated += 1\n",
    "        elif len(sample) < maxlen:\n",
    "            padded += 1\n",
    "        else:\n",
    "            exact +=1 \n",
    "    print('Padded: {}'.format(padded))\n",
    "    print('Equal: {}'.format(exact))\n",
    "    print('Truncated: {}'.format(truncated))\n",
    "    print('Avg length: {}'.format(total_len/len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded: 22560\n",
      "Equal: 12\n",
      "Truncated: 2428\n",
      "Avg length: 202.43204\n"
     ]
    }
   ],
   "source": [
    "dataset = pre_process_data(imdb_datasets)\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "\n",
    "test_len(vectorized_data, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，每个document 的平均长度是202 个tokens. 所以400 的设置可能带来了过多的dirty data。我们下面尝试200.\n",
    "\n",
    "#### 训练一个较小的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data)*.8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))  # 20000 * 200 * 300\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 200, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 200, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 80,201\n",
      "Trainable params: 80,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_neurons = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 536s 27ms/step - loss: 0.4771 - acc: 0.7718 - val_loss: 0.4094 - val_acc: 0.8134\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 515s 26ms/step - loss: 0.3643 - acc: 0.8420 - val_loss: 0.3453 - val_acc: 0.8610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fff054940>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存这个较小的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"lstm_model_200.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"lstm_weights_200.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上可见：\n",
    "- Network 的参数比之前的小\n",
    "- 我们的network unroll 200 次，所以相比400次训练更快。\n",
    "- validation acc 增加到0.86，说明移除dirty data 对提升准确率有一些作用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Unkown tokens\n",
    "\n",
    "前面我们讲了padding的问题，还有一个问题是，我们丢弃了unknown words. 在做词向量的时候，如果遇到了lexicon 中没有的词，会丢弃。有的时候，可能会造成语义的误解，例如：\n",
    "\n",
    "I dont like this movie.\n",
    "\n",
    "其中单词dont 是一个非正规的缩写，所以在lexicon 中可能找不到，如果丢弃了之后变成：\n",
    "\n",
    "I like this movie.\n",
    "\n",
    "这就造成了语义上的问题。\n",
    "\n",
    "下面我们介绍两个通用的方法，这两个方法都是寻找一个vector representation 来替换这个不在字典里的词。\n",
    "\n",
    "#### 1. 随机选一个词替换\n",
    "\n",
    "这听上去有些反人类(counter-intuitive), 因为随便替换了之后，即便是人可能也无法理解意思。但其实这并不是一个问题，因为我们的目标是追求泛化，所以只有一两个dirty data 对于模型的影响不大。\n",
    "\n",
    "#### 2. 用UNK 替换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Character-level LSTM\n",
    "\n",
    "单词并不是表示meaning 的最小语义单元，我们有时候需要寻找更小的building block, 例如stems，phonemes，etc.\n",
    "\n",
    "下面我们试图用LSTM 建立一个character level 的模型。在这个模型中，每个标点都被当作一个character. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pre_process_data(imdb_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看一下平均每个document 有多少个字母，结果显示是1325个。\n",
    "\n",
    "**对于参数的选择，观察数据很重要**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_len(data):\n",
    "    total_len = 0\n",
    "    for sample in data:\n",
    "        total_len += len(sample[1])\n",
    "    return total_len/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325.06964\n"
     ]
    }
   ],
   "source": [
    "avg_len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看出，我们的network 需要unroll 1000多次，所以训练起来可能需要更长的时间。\n",
    "\n",
    "### 3.2 数据清洗\n",
    "\n",
    "character 和token 的区别在于，输入我们的lexicon 只有26个字母 + 10个数字 + 有限的特殊符号，所以input vector 的dimension 会比token based 小。下面，我们使用一个方法来识别有效的character，不在这个范围内的character 我们使用UNK 来代替。**注意，这里UNK 看作一个character**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\" Shift to lower case, replace unknowns with UNK, and listify \"\"\"\n",
    "    new_data = []\n",
    "    VALID = 'abcdefghijklmnopqrstuvwxyz123456789\"\\'?!.,:; '\n",
    "    for sample in data:\n",
    "        new_sample = []\n",
    "        for char in sample[1].lower():  # Just grab the string, not the label\n",
    "            if char in VALID:\n",
    "                new_sample.append(char)\n",
    "            else:\n",
    "                new_sample.append('UNK')\n",
    "       \n",
    "        new_data.append(new_sample)\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "listified_data = clean_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Padding\n",
    "\n",
    "同上，我们做padding，对于不到max_len 的documents，使用特殊的token \"UNK\" 来填补。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_pad_trunc(data, maxlen):\n",
    "    \"\"\" We truncate to maxlen or add in PAD tokens \"\"\"\n",
    "    new_dataset = []\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            new_data = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            pads = maxlen - len(sample)\n",
    "            new_data = sample + ['PAD'] * pads\n",
    "        else:\n",
    "            new_data = sample\n",
    "        new_dataset.append(new_data)\n",
    "    return new_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 1500\n",
    "\n",
    "common_length_data = char_pad_trunc(listified_data, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_length_data[0])  # check the length after padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 构建字典\n",
    "\n",
    "之前我们使用了word2vec，相当于一个将单词转化为vector 的词典，我们现在要手动构建一个类似功能的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(data):\n",
    "    \"\"\" Modified from Keras LSTM example\"\"\"\n",
    "    chars = set()\n",
    "    for sample in data:\n",
    "        chars.update(set(sample))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    return char_indices, indices_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices, indices_char = create_dicts(common_length_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 构建one-hot 编码\n",
    "\n",
    "接下来，我们使用字典，来创建input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_encode(dataset, char_indices, maxlen):\n",
    "    \"\"\" \n",
    "    One hot encode the tokens\n",
    "    \n",
    "    Args:\n",
    "        dataset  list of lists of tokens\n",
    "        char_indices  dictionary of {key=character, value=index to use encoding vector}\n",
    "        maxlen  int  Length of each sample\n",
    "    Return:\n",
    "        np array of shape (samples, tokens, encoding length)\n",
    "    \"\"\"\n",
    "    X = np.zeros((len(dataset), maxlen, len(char_indices.keys())))\n",
    "    for i, sentence in enumerate(dataset):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = onehot_encode(common_length_data, char_indices, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 split dataset into training / testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(encoded_data)*.8)\n",
    "\n",
    "x_train = encoded_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = encoded_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 define netowrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 1500, 20)          5360      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1500, 20)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 30001     \n",
      "=================================================================\n",
      "Total params: 35,361\n",
      "Trainable params: 35,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, LSTM\n",
    "\n",
    "num_neurons = 20\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, len(char_indices.keys()))))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 2157s 108ms/step - loss: 0.6903 - acc: 0.5443 - val_loss: 0.6857 - val_acc: 0.5542\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 2209s 110ms/step - loss: 0.6325 - acc: 0.6429 - val_loss: 0.6685 - val_acc: 0.5922\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 2190s 110ms/step - loss: 0.5887 - acc: 0.6865 - val_loss: 0.6818 - val_acc: 0.5886\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 2367s 118ms/step - loss: 0.5521 - acc: 0.7156 - val_loss: 0.6961 - val_acc: 0.5932\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 2356s 118ms/step - loss: 0.5191 - acc: 0.7429 - val_loss: 0.7130 - val_acc: 0.5842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a41bce0f0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结果分析\n",
    "\n",
    "训练了5个epoch 之后，我们的训练集准确度是74%，但是validation accuracy 只有58% 左右。\n",
    "如果训练10个epoch，training accuracy 可以到90+%，但是validation accuracy 还是一半多一些，说明了overfitting。\n",
    "\n",
    "\n",
    "⚠️ 这个结果有问题\n",
    "\n",
    "一个原因是，我们的模型相对于之歌较小的数据集是过于复杂了，所以泛化能力不够。可能的解决方法：\n",
    "- 增加dropout percentage: 不要超过50%\n",
    "- 每一层使用更少的neuron 个数\n",
    "- 提供更多的数据 - expensive to get\n",
    "\n",
    "⚠️ **Question**: character-level 的模型又慢又不准，我们为什么还要介绍它？\n",
    "\n",
    "我们要训练一个这样的模型，需要更大的training set，当前我们使用的IMDB 的training set 太小。如果能有一个更大的dataset，character-level model 的准确率会很好。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extensions \n",
    "\n",
    "### 4.1 Other kinds of memory\n",
    "\n",
    "其他的memory 在gate 的operation 会有稍许区别。\n",
    "\n",
    "#### GRU\n",
    "\n",
    "更高效：更少的参数\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(num_neurons, return_sequences=True, input_shape=X[0].shape))\n",
    "```\n",
    "\n",
    "#### peephole connections\n",
    "\n",
    "`Learning Precise Timing with LSTM Recurrent Networks`\n",
    "\n",
    "区别在于，input 现在变成三个信号的叠加：\n",
    "- input at time t\n",
    "- output at time t-1\n",
    "- memory state\n",
    "\n",
    "对于time series data 比较有效\n",
    "\n",
    "### 4.2 Going deeper\n",
    "\n",
    "叠加(stack)多个LSTM 层, 注意，第一层和中间层的`return_sequences=True`.\n",
    "\n",
    "<img src=\"img/stacked_lstm.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape))\n",
    "model.add(LSTM(num_neurons_2, return_sequences=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "- Remembering information with memory units enables more accurate and general models of the sequence.\n",
    "- It’s important to forget information that is no longer relevant.\n",
    "- Only some new information needs to be retained for the upcoming input, and LSTMs can be trained to find it.\n",
    "- If you can predict what comes next, you can generate novel text from probabilities.\n",
    "- Character-based models can more efficiently and successfully learn from small, focused corpora than word-based models.\n",
    "- LSTM thought vectors capture much more than just the sum of the words in a statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
