{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "语言描述能力真正强大的地方不仅在于每个单词的意思，更重要的在于order and combination of words. The connection between words creates depth, information and complexity.\n",
    "\n",
    "Meaning is hidden beneath the words.\n",
    "\n",
    "下面我们要介绍，如何获取**latent semantic information** (meaning, emotion, etc.) from a sequence of words. \n",
    "\n",
    "人类的语言和机器生成的语言的差别在于tone and flow.\n",
    "\n",
    "## 1. Learning Meaning\n",
    "\n",
    "一个单词的意义很大程度上取决于和周边词的关系(relationship)。这种关系包括：\n",
    "\n",
    "1. word order\n",
    "2. word proximity\n",
    "\n",
    "relationship 可以有两种建模方法：\n",
    "\n",
    "1. spatially: as writing\n",
    "    - viewd through a fixed-width window\n",
    "2. temporarily: as spoken -- time series data\n",
    "    - extend for an unknown amount of time\n",
    "    \n",
    "传统neural nets (e.g., fully connected neural nets) 的强项是从数据中提取pattern，但是不能提取token 之间的relation。之后我们要介绍一些能够捕捉关系的神经网络：\n",
    "- CNN\n",
    "- RNN\n",
    "\n",
    "<img src=\"img/fully_connected_NN.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "## 2. CNN Review\n",
    "\n",
    "**Convolution**: the concept of sliding (or convolving) a small window over the data sample.\n",
    "\n",
    "#### kernel (filter):\n",
    "- 3*3\n",
    "- random initialization with small numbers (close to 0)\n",
    "- can have n filters -- output n \"images\"\n",
    "\n",
    "#### stride (step size):\n",
    "- 小于filter size，保证每个snapshot有overlap\n",
    "\n",
    "#### padding\n",
    "\n",
    "adding enough data to the input’s outer edges so that the first real data point is treated just as the innermost data points are.\n",
    "\n",
    "- why padding?\n",
    "    - without padding, 输出的size 和输入的size 不同\n",
    "    - 图片边界部分undersampling, 因为只有一个filter\n",
    "        - 对于tweets 这样的短文，undersampling 影响很大\n",
    "        \n",
    "- padding 的方法\n",
    "    - valid: 不加padding\n",
    "    - same: 用相同的元素补齐\n",
    "    - guess at what the padding should be: 适用于图片，不适用于文本\n",
    "\n",
    "- padding 可能带来的问题：\n",
    "    - adding potentially unrelated data to the input, which in itself can skew the outcome\n",
    "    \n",
    "#### pipeline\n",
    "- 多个conv layer 可以拼接\n",
    "- 最后一个conv layer 需要展开，连成一个vector，然后接入fully connected layer, 然后做分类等特性。\n",
    "\n",
    "#### filter composition\n",
    "每snapshot 都和其他的snapshot 无关，所以可以最大程度的使用CPU 的并行机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN for text\n",
    "\n",
    "Use 1-dimensional CNN 来检测IMDB 电影评价数据集。\n",
    "- 每个负面的评论被标记为0\n",
    "- 每个正向的评论被标记为1\n",
    "\n",
    "下载数据集：https://ai.stanford.edu/%7eamaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: 读取IMDB数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_datasets = '/Users/chenwang/Workspace/datasets/IMDB/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    \"\"\"\n",
    "    This is dependent on your training data source but we will try to generalize it as best as possible.\n",
    "    \"\"\"\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "    \n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "            \n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pre_process_data(imdb_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)  # 一共25000 条评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'The anime that got me hooked on anime...<br /><br />Set in the year 2010 (hey, that\\'s not too far away now!) the Earth is now poison gas wasteland of pollution and violence. Seeing as how crimes are happening ever 30 seconds are so and committed by thieves who have the fire power of third world terrorists, the government of the fictional New Port City form the Tank Police to deal with the problem - cops with tanks! Oh the insanity!<br /><br />The \"heroes\" of this series include the new recruit Leona Ozaki, a red haired Japanese woman (yeah I know, they never match their distinctly Japanese names with a Japanese appearance) who has just been drafted into the Tank Police and is quickly partnered with blond, blue eyed nice guy Al. Leona is new at using tanks and unfortunately she destroys the favorite tank of Tank Police Commander Charles Britain (also known as \"Brenten\"), a big guy who looks like Tom Selleck on steroids and sporting a pair of nifty sunglasses, a big revolver and a bad temper. Britain didn\\'t like having Leona join the Tank Police in the first place and her wrecking his Tiger Special (a giant green monster tank) doesn\\'t exactly endear her to him, nor is he fond of her taking the remains of his giant tank and using it to build a mini-tank that she nicknames Bonaparte and he is soon pushing to have her transferred to child welfare \"where the boys are more your size\" as he puts it. There\\'s also Specs, the bifocal genius, Bible quoting/God fearing Chaplain, purple MO-hawked Mohican, and the pot bellied Chief, who\\'s right on the edge thanks to the Mayor always yelling at him about the Tank Police antics. Seeing as how the tank cops often destroy half the city while chasing the bad guys and use extreme violence to capture them, they\\'re not very well liked by the people.<br /><br />The \"villains\" are a cyborg named Buaku who\\'s got a mysterious past that\\'s connected with a project known as \"Green Peace\", his gang and his two sexy cat cyborg sidekicks Anna & Uni Puma. In the first installment these guys are being paid to steal urine samples from a hospital treating people who haven\\'t been infected by the poison gas clouds and in the 2nd they\\'re hired to steal a painting that is of a naked Buaku. The story, however, was uncompleted in the anime and was finished up in a cult comic (\"Manga\") book that\\'s very hard to find.<br /><br />All sorts of chaos and mayhem ensue in this black comic venture that examines how far people want their police to go in order to catch criminals and what happens when the fine line between good guys and bad guys starts to get blurred. This is the kind of thing that if you were going to make a movie of it, you\\'d better go get Quentin Tarantino. Uneven in places but still a lot of fun.<br /><br />Followed by \"New Dominion: Tank Police\".')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]  # 第一条评论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('/Users/chenwang/Workspace/datasets/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        # sample 是一个二元组，sample[0] 是label, sample[1] 是评论text\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        \n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "        \n",
    "        # list of list    \n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3. split training and testing sets\n",
    "\n",
    "以及shuffle 过，所以直接取前80%elements 作为training set，剩余的作为testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data)*.8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4. set up CNN parameters\n",
    "\n",
    "CNN 网络的输入维度都是相同的。所以要使用maxlen 参数，对于超长的review 进行截断，对于短的review 进行padding。padding 可以是Null 或者0. padding = \"ignore me\". \n",
    "\n",
    "⚠️ 这个padding 和上面CNN 中讲的padding 不同：\n",
    "- CNN 中的padding 是为了避免边缘输入subsampling\n",
    "- 这里的padding 是为了输入数据保持相同的尺寸\n",
    "\n",
    "kernel_size = 3 means looking at 3-grams of your input text.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400            # max review length\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "filters = 250           # Number of filters we will train\n",
    "kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
    "hidden_dims = 250       # Number of neurons in the plain feed forward net at the end of the chain\n",
    "epochs = 2              # Number of times we will pass the entire training dataset through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5: preprocessing: padding & truncate\n",
    "\n",
    "下面函数可以用一行list comprehension 替换：\n",
    "\n",
    "```python\n",
    "\n",
    "[smp[:maxlen] + [[0.] * emb_dim] * (maxlen - len(smp)) for smp in data]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    \n",
    "    zero_vector = []\n",
    "    \n",
    "    # data[0] 第一个review 所有token 的词向量\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    " \n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1][200]  # 查看补全的全龄向量 -- 第2个review 的第200个token 的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步，转化成numpy array，作为keras 的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))  # 20000 * 400 * 300\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 400, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape  # 20000 条评论，每个评论400个tokens，每个token 是一个长度为300 的vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 6. CNN architecture\n",
    "\n",
    "- `padding = valid` 也就是不需要padding 即输出比输入的dimension 要小。\n",
    "- `strides=1` 每次一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "# standard model definition pattern for keras\n",
    "model = Sequential()\n",
    "\n",
    "# we add a Convolution1D, which will learn word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1,\n",
    "                 input_shape=(maxlen, embedding_dims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pooling\n",
    "\n",
    "two methods for pooling:\n",
    "- max\n",
    "- average\n",
    "\n",
    "we use max pooling: let networks see the most prominent feature of each subsection.\n",
    "\n",
    "default size of pooling window is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, 对每一个input review，我们都有一个1D 的vector 来represent that sample. 这个vector 可以看作semantic representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout\n",
    "\n",
    "goal: avoid overfitting\n",
    "\n",
    "通过randomly \"turn off\" 一些neurons，来减少参数，避免overfitting。被turn off 的neuron 相当于output 0，因此这些neurons 对cost function 的贡献相当于0， 因此相应的weights 在backpropagation 中不会被更新。\n",
    "\n",
    "因为turn off 了一些neurons，所以整个neural networks 的signal 强度减小。所以keras 会自动按比例boosting 没有被turn off 的neurons。\n",
    "\n",
    "⚠️ 在inference/prediction 的过程中不使用dropout。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer\n",
    "\n",
    "Actual classifier, 这里我们使用sigmoid function。 \n",
    "\n",
    "对于多分类问题，最后的output layer 可以如下定义。\n",
    "\n",
    "```python\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络结构总结 (每一层的维度信息)\n",
    "\n",
    "- 生成doc matrix:\n",
    "    - 对于每个document，做tokenization，不足400个token 的补齐，多于400个tokens 的剪切。所以每个document 最后变成400 个token 的。(注意，在下图中没有展现padding 这一步)\n",
    "    - 每个token做word embedding (或者其他方式的向量化)，则每个document 转变成一个embedding_dim * maxlen 的matrix， 例如下图所示，每个embedding 是6维向量，因此doc matrix 是6 * 9\n",
    "- 生成 filter matrix: embedding_dim * kernel_size, 例如下图所示，6 * 3\n",
    "- filter matrix 和document matrix 做convole。生成一下较小的vector，如下图所示，生成 1 * 7 的vector\n",
    "\n",
    "<img src=\"img/cnn_nlp.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "- global max pooling: 取这个vector 的最大值, 所以得到一个数，如下图所示：\n",
    "\n",
    "<img src=\"img/1d_max_pooling.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "- \"thought vector\": 我们有`filters` 个filters，每个filter 如上述步骤，生成一个数，所以我们最后会等得到一个长度为`filters` 的向量，假设我们有250 个filters，我们会得到一个1 * 250 的向量。\n",
    "\n",
    "- fully connected network (可以使用dropout)\n",
    "    - 输入是thought vector 的长度，即filter 的个数\n",
    "    - 输出是hidden_dims 个neuron\n",
    "    - 使用dropout(0.2)\n",
    "    \n",
    "- output layer\n",
    "    - 输入是hidden_dims\n",
    "    - 输出是1 (0/1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 7: compile model\n",
    "\n",
    "- loss function\n",
    "    - binary_crossentropy: 1个neuron，输出 0 or 1\n",
    "    - categorical_crossentropy: n个neurons，输出 one-hot vector\n",
    "- optimizer: algorithms to minimize the cost function\n",
    "    - Stochastic gradient descent\n",
    "    - Adam\n",
    "    - RSMProp\n",
    "\n",
    "- metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 8: train\n",
    "\n",
    "- `compile` - build model\n",
    "- `fit` - train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 626s 31ms/step - loss: 0.3874 - acc: 0.8196 - val_loss: 0.2972 - val_acc: 0.8758\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 525s 26ms/step - loss: 0.2307 - acc: 0.9077 - val_loss: 0.2764 - val_acc: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a61c136a0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果来看，model 有些overfitting，training acc: 0.9077，validation acc: 0.8838. \n",
    "\n",
    "但是overfiting 并不严重，因为both traning acc 和validation acc 都在涨。如果training acc 在提高，validation acc 在降低，则是一个strong sign of overfitting.\n",
    "\n",
    "如果model 还在内存中，我们可以在训练一个epoch，直接调用`fit`方法。如果model 不在内存中，我们可以load from disk (step 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 530s 27ms/step - loss: 0.1116 - acc: 0.9604 - val_loss: 0.3316 - val_acc: 0.8736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a45ce0278>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，我们再训练一个epoch，出现了overfitting 的情况，因为training acc 提升，validation acc 降低。\n",
    "\n",
    "我们下面看如何使用模型进行prediction。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 9. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass a dummy value in the first element of the tuple \n",
    "# just because our helper expects it from the way processed the initial data.  \n",
    "# That value won't ever see the network, so it can be whatever.\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "\n",
    "# Tokenize returns a list of the data (length 1 here)\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `predict` 方法返回raw data，即output layer sigmoid 方法的输出\n",
    "- `predict_classes` 方法返回class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01506066]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_vec)  # 0 is negative comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 10: save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"cnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"cnn_weights.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了保证每次一样，可以事先设置一个随机数种子。\n",
    "\n",
    "下面两行加在model definition 之前。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 11: load a saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "with open(\"cnn_model.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "model.load_weights('cnn_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 总结\n",
    "\n",
    "#### 为什么使用CNN？\n",
    "efficiency: dropout 丢失和很多数据，更有效\n",
    "\n",
    "#### how to improve?\n",
    "- 多个conv1D layer stack together\n",
    "- 使用不同长度的filters，然后把结果拼成一个更长的vector (thought vector)，再传入output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
