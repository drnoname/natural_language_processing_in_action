{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04: Semantic Analysis\n",
    "\n",
    "之前我们通过TF-IDF 来评估每个word 的重要程度。TF-IDF vectors and matrices tell how important each word is to the\n",
    "overall meaning of a bit of text in a document collection.\n",
    "\n",
    "#### Latent Semantic Analysis (LSA)\n",
    "algorithm for revealing the meaning of word combinations and computing vectors to represent this meaning.\n",
    "\n",
    "本章主要介绍，如何使用TF-IDF vector 来计算topic vector / semantic vector. \n",
    "- group words together in topics\n",
    "- the linear combinations of words that make up the dimensions of your topic vectors\n",
    "\n",
    "使用topic vector 的优势在于：\n",
    "- semantic search: search based on meaning -- better than keyword search\n",
    "- 摘要：the most meaningful words for a document — a set of keywords that summarizes its meaning.\n",
    "- compare two documents to check how close they are in meaning to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From word counts to topic scores \n",
    "\n",
    "### 1.1 TF-IDF 的局限性\n",
    "\n",
    "TF-IDF 只是frequency, nothing else (cannot tell the meaning).\n",
    "\n",
    "texts that restate the same meaning will have completely different TF-IDF vector representations if they\n",
    "spell things differently or use different words.  -- document similarity comparisons 有时会出问题 相似的documents 必须使用相似的tokens，这点要求有些苛刻。\n",
    "\n",
    "stemming / lemmatization: similar spelling, similar meaning. 相当于根据spelling 做了一个clustering，然后用stem 或lemma 来代替这一个cluster 的tokens.\n",
    "\n",
    "- 问题是这两种方法只能识别拼写相近的token，但是很难找到近义词(Synonyms). False Negative -- 相似的找不到\n",
    "- 有时，会把反义词(antonyms)判断成同义词，因为拼写十分相近。 False Positive -- 找到的不相似\n",
    "\n",
    "所以，内容相近的documents，因为使用了不同的词，他们在TF-IDF vector space 的距离可能很远。除此以为，对这些vector 进行+/- 操作，得到的结果也不是很有意义(compare to word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Topic Vectors\n",
    "\n",
    "我们需要得到additional information -- meaning, or topic. \n",
    "- estimate of what the words in a document “signify.”\n",
    "- what that combination of words means in a particular document.\n",
    "\n",
    "然后把这个meaning 转变成一个more compact, meaningful vector - topic vector.\n",
    "- 对topic vector 进行+/- 操作很有意义；\n",
    "- distance between vector is useful for clustering documents or semantic search\n",
    "- 每个token 有一个word topic vector，基于此，计算每个document 的document topic vector。\n",
    "- 可以给每个word 一个权值，权值可以为负\n",
    "\n",
    "#### Challenges\n",
    "- polysemy: The existence of words and phrases with more than one meani\n",
    "    - many different interpretations of the same words\n",
    "- Homonyms： \n",
    "    - text: Words with the same spelling and pronunciation, but different meanings\n",
    "    - speech: Words spelled the same, but with different pronunciations and meanings\n",
    "- Zeugma： \n",
    "    - text: Use of two meanings of a word simultaneously in the same sentence\n",
    "    - speech: Words with the same pronunciation, but different spellings and meanings (an NLP challenge with voice interfaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Thought Experiment\n",
    "\n",
    "⚠️这不是一个real algorithm or implementation，只是一个思考问题解决方法的办法。\n",
    "\n",
    "think: 每个单词对topic 的贡献度\n",
    "\n",
    "假设我们有三个topics：\n",
    "- pets\n",
    "- animals\n",
    "- cities\n",
    "\n",
    "假设我们的lexicon 包括以下几个单词：\n",
    "\n",
    "```python\n",
    "['cat', 'dog', 'apple', 'lion', 'NYC', 'love']\n",
    "```\n",
    "\n",
    "我们可以对每个单词对每个topic 的贡献度设置一个权值，然后通过weighted the word frequencies 来计算topic vector.\n",
    "\n",
    "**weight**: how likely the word is associated with a topic\n",
    "\n",
    "```python\n",
    ">>> topic['petness'] = (.3 * tfidf['cat'] +\\\n",
    "... .3 * tfidf['dog'] +\\\n",
    "... 0 * tfidf['apple'] +\\\n",
    "... 0 * tfidf['lion'] -\\\n",
    "... .2 * tfidf['NYC'] +\\\n",
    "... .2 * tfidf['love'])\n",
    ">>> topic['animalness'] = (.1 * tfidf['cat'] +\\\n",
    "... .1 * tfidf['dog'] -\\\n",
    "... .1 * tfidf['apple'] +\\\n",
    "... .5 * tfidf['lion'] +\\\n",
    "... .1 * tfidf['NYC'] -\\\n",
    "... .1 * tfidf['love'])\n",
    ">>> topic['cityness'] = ( 0 * tfidf['cat'] -\\\n",
    "... .1 * tfidf['dog'] +\\\n",
    "... .2 * tfidf['apple'] -\\\n",
    "... .1 * tfidf['lion'] +\\\n",
    "... .5 * tfidf['NYC'] +\\\n",
    "... .1 * tfidf['love'])\n",
    "```\n",
    "    \n",
    "这个weight matrix 可以flipped (transposed).\n",
    "\n",
    "```python\n",
    ">>> word_vector = {}\n",
    ">>> word_vector['cat'] = .3*topic['petness'] +\\\n",
    "... .1*topic['animalness'] +\\\n",
    "... 0*topic['cityness']\n",
    ">>> word_vector['dog'] = .3*topic['petness'] +\\\n",
    "... .1*topic['animalness'] -\\\n",
    "... .1*topic['cityness']\n",
    ">>> word_vector['apple']= 0*topic['petness'] -\\\n",
    "... .1*topic['animalness'] +\\\n",
    "... .2*topic['cityness']\n",
    ">>> word_vector['lion'] = 0*topic['petness'] +\\\n",
    "... .5*topic['animalness'] -\\\n",
    "... .1*topic['cityness']\n",
    ">>> word_vector['NYC'] = -.2*topic['petness'] +\\\n",
    "... .1*topic['animalness'] +\\\n",
    "... .5*topic['cityness']\n",
    ">>> word_vector['love'] = .2*topic['petness'] -\\\n",
    "... .1*topic['animalness'] +\\\n",
    "... .1*topic['cityness']\n",
    "```\n",
    "\n",
    "上面的6个vector，代表了在三维空间(topic vector space) 中的点, 如下图所示。\n",
    "\n",
    "<img src=\"img/topic_vectors.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "在这个例子中，我们把6D向量压缩到3D向量。原先每个token 是一个6D 向量，6是lexicon 的大小。所以在真实场景中，可能是10000D。3是topic 个数，在真实场景中，可能也只有100.\n",
    "\n",
    "\n",
    "#### 下一步：这些weight 怎么设置？\n",
    "    \n",
    "上面的例子中，我们手动设置weight matrix，缺陷明显：\n",
    "- labor-intensive\n",
    "- Common sense is hard to code into an algorithm.\n",
    "    - 数据多了也不知道该assign 多少weight\n",
    "- 多少个topic？\n",
    "\n",
    "实际上，我们重新看这个问题，发现他就是一个降维的问题。transfer a vector from a higher vector space (TF-IDF space) to a lower-dimensional vector space (topic space).\n",
    "\n",
    "- inputs:\n",
    "    - weight matrix: 3*6 (3 topics, 6 tokens in the lexicon)\n",
    "    - TF-IDF vector: 6*1\n",
    "- outputs:\n",
    "    - topic vector: 3*1\n",
    "\n",
    "下面，我们来看具体的算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 An algorithm for scoring topics\n",
    "\n",
    "**You shall know a word by the company it keeps.**\n",
    "\n",
    "**company** = co-occurrences\n",
    "\n",
    "**LSA (Latent Semantic Analysis)** is an algorithm to analyze your TF-IDF matrix (table of TF-IDF vectors) to gather up words into topics. \n",
    "- It works on bag-of-words vectors, too, but TF-IDF vectors give slightly better results.\n",
    "- maintain diversity\n",
    "- LSA is often referred to as a dimension reduction technique\n",
    "    - PCA is exactly the same math as LSA\n",
    "    - LSA: 用于语义分析的PCA\n",
    "    - in the field of information retrieval, focus is to create index for search, LSA = LSI (Latent Semantic Indexing)\n",
    "    \n",
    "    \n",
    "two algorithms are similar to LSA:\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Latent Dirichlet allocation (LDiA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 An LDA Classifier\n",
    "\n",
    "LDA 简单，明了，我们介绍当作一个warm up. 后面我们来看一些更fancy 的方法。\n",
    "\n",
    "- supervised, need labels\n",
    "- 需要的sample 个数不多\n",
    "\n",
    "The model “training” has only three steps：\n",
    "1. Compute the average position (centroid) of all the TF-IDF vectors within the class (such as spam SMS messages).\n",
    "2. Compute the average position (centroid) of all the TF-IDF vectors not in the class (such as nonspam SMS messages).\n",
    "3. Compute the vector difference between the centroids (the line that connects them).\n",
    "\n",
    "**核心**：\n",
    "- training: to find the vector (line) between the two centroids for your binary class\n",
    "- classifying: 离哪个class 的centroid 更近\n",
    "\n",
    "下面，我们用LDA Classifier 来做一个最简单的饿spam filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.width = 120\n",
    "\n",
    "sms = pd.read_csv('data/sms-spam.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['spam'] = sms.spam.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4837"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sms)  # 一共有4837 个messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.spam.sum()  # 一共有638 个message 被标记成spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text\n",
       "0     0  Go until jurong point, crazy.. Available only ...\n",
       "1     0                      Ok lar... Joking wif u oni...\n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3     0  U dun say so early hor... U c already then say...\n",
       "4     0  Nah I don't think he goes to usf, he lives aro...\n",
       "5     1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6     0  Even my brother is not like to speak with me. ...\n",
       "7     0  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8     1  WINNER!! As a valued network customer you have...\n",
       "9     1  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. tokenization and TF-IDF vector transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=sms.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4837, 9232)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.spam.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看出，tokenization 之后，lexicon 有9232 个token。所以现在特征的个数大于样本的个数. 在这种情况下，一些分类器（例如Naive Bayes Classifier）的表现不好。在这种情况下，可以使用semantic analysis techniques 的方法。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 计算两个class 的centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sms.spam.astype(bool).values\n",
    "\n",
    "spam_centroid = tfidf_docs[mask].mean(axis=0)\n",
    "ham_centroid = tfidf_docs[~mask].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06, 0.  , 0.  , ..., 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_centroid.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.01, 0.  , ..., 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_centroid.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 计算连接两个centroids 的一个vector\n",
    "\n",
    "from ham centroid to spam centroid\n",
    "\n",
    "model_vec: The arrow from the nonspam centroid to the spam centroid is the line that defines your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.39266024e-02, -1.92685506e-03,  3.84287194e-04, ...,\n",
       "       -6.31869803e-05, -6.31869803e-05, -6.31869803e-05])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vec = spam_centroid - ham_centroid\n",
    "model_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. 对已知样本进行分类\n",
    "\n",
    "每一个document 是一个TF-IDF vector. 然后用这个vector 和model_vec 做点乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01469806, -0.02007376,  0.03856095, ..., -0.01014774,\n",
       "       -0.00344281,  0.00395752])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamminess_score = tfidf_docs.dot(model_vec)\n",
    "spamminess_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06904539440075813"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(spamminess_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03935727183816804"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(spamminess_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalization\n",
    "\n",
    "我们希望这个score 在0 - 1 之间，类似于概率，所以我们要做standardization - MaxMinScaler\n",
    "\n",
    "然后当这个normalized score 大于0.5， 我们预测为spam，否则预测为nonspam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sms['lda_score'] = MinMaxScaler().fit_transform(spamminess_score.reshape(-1,1))\n",
    "sms['lda_predict'] = (sms.lda_score > .5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "      <th>lda_score</th>\n",
       "      <th>lda_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0.227478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0.177888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>0.718785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0.184565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0.286944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>0.548003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0.324953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0.499636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>0.892853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>0.766372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text  lda_score  lda_predict\n",
       "0     0  Go until jurong point, crazy.. Available only ...   0.227478            0\n",
       "1     0                      Ok lar... Joking wif u oni...   0.177888            0\n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   0.718785            1\n",
       "3     0  U dun say so early hor... U c already then say...   0.184565            0\n",
       "4     0  Nah I don't think he goes to usf, he lives aro...   0.286944            0\n",
       "5     1  FreeMsg Hey there darling it's been 3 week's n...   0.548003            1\n",
       "6     0  Even my brother is not like to speak with me. ...   0.324953            0\n",
       "7     0  As per your request 'Melle Melle (Oru Minnamin...   0.499636            0\n",
       "8     1  WINNER!! As a valued network customer you have...   0.892853            1\n",
       "9     1  Had your mobile 11 months or more? U R entitle...   0.766372            1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果我们可以看出，前10条数据我们都预测正确了。下面，我们来统计一下对于所有样本的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = sms['spam'].tolist()\n",
    "predict = sms['lda_predict'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[4135   64]\n",
      " [  45  593]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "  \n",
    "results = confusion_matrix(label, predict) \n",
    "  \n",
    "print('Confusion Matrix :')\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.9774653710977879\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score :',accuracy_score(label, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      4199\n",
      "           1       0.90      0.93      0.92       638\n",
      "\n",
      "    accuracy                           0.98      4837\n",
      "   macro avg       0.95      0.96      0.95      4837\n",
      "weighted avg       0.98      0.98      0.98      4837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Report : ')\n",
    "print(classification_report(label, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a very simple model, with few parameters, so it should generalize well.\n",
    "\n",
    "### 1.6 LDiA: latent Dirichlet allocation\n",
    "\n",
    "#### limitations\n",
    "- take longer time to traing\n",
    "- less practical for many real-world applications\n",
    "\n",
    "#### advantages\n",
    "- topics are easier to intepret\n",
    "\n",
    "#### 使用场景\n",
    "- document summarization: \n",
    "    - identify the most \"central\" sentences of a document\n",
    "    - put sentences together to create a machine-generated summary\n",
    "    \n",
    "- 对于clasification 和regression problems, LSA is better.\n",
    "\n",
    "#### tools\n",
    "- gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Latent Semantic Analysis\n",
    "\n",
    "基于SVD (singular value decomposition). SVD 的一个应用场景是\"**matrix inversion**\". A matrix can be inverted by decomposing it into three simpler square matrices, transposing matrices, and then multiplying them back together.\n",
    "\n",
    "Latent semantic analysis is a mathematical technique for finding the “best” way to linearly transform (rotate and stretch) any set of NLP vectors, like your TF-IDF vectors or bag-of-words vectors.\n",
    "\n",
    "Eliminate those dimensions in the new vector space that don’t contribute much to the variance in the vectors from document to document.\n",
    "\n",
    "some tricks that help improve the accuracy of LSA vectors\n",
    "\n",
    "The machine doesn’t “understand” what the combinations of words means, just that they go together. When it sees words like “dog,” “cat,” and “love” together a lot, it puts them together in a topic. It doesn’t know that such a topic is likely about “pets.” 我们需要给这每个topic 一个name. \n",
    "\n",
    "#### Awas! Awas! Tom is behind you! Run!\n",
    "\n",
    "从上下文我们可以大致猜出Awas 的意思。有点儿类似于我们小时候做的填空题。\n",
    "\n",
    "just focusing on the language context, the words, you can often “transfer” a lot of the significance or meaning of words you do know to words that you don’t.\n",
    "\n",
    "document 通常是一句话，而不是更长的单位。原因是the meaning of a word is usually closely related to the meanings of the words in the sentence that contains it. 联想word2vec 的窗口大小设置为5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
