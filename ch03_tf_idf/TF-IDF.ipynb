{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch03 TF-IDF\n",
    "\n",
    "Which words are more important to a particular document and across the corpus as a whole?\n",
    "\n",
    "If you have an idea about the frequency with which those words appear in a document in relation to the rest of the documents, you can use that to further refine the “positivity” of the document.\n",
    "\n",
    "之前，每个word vector 都是一个binary vector, 每一位都是0/1,表示absence / presence。下一步，用continuous number。\n",
    "\n",
    "TF-IDF: term frequency times inverse document frequency\n",
    "- Term Frequency: the number of times a word occurs in a given document\n",
    "    - 有时需要normalize: divided by the number of tokens in the document\n",
    "    - normalize 反映了 relative importance to the document of that term\n",
    "- Inverse Document Frequency: divide each of those word counts by the number of documents in which the word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get unique words\n",
    "\n",
    "我们使用Counter，是一个unordered collection, 也称为bag or multiset.bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 和, 没有什么information，所以可以忽略。\n",
    "\n",
    "下面，我们计算harry 的TF. \n",
    "\n",
    "⚠️这里为什么是除以unique words 的个数，而不是document 的单词个数？这样除有可能大于1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: 2\n",
      "number of words: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears = bag_of_words['harry']  # harry 计数\n",
    "print('counter: {}'.format(times_harry_appears))\n",
    "num_unique_words = len(bag_of_words)  # \n",
    "print('number of words: {}'.format(num_unique_words))\n",
    "tf = times_harry_appears / num_unique_words\n",
    "round(tf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_kite = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react\n",
    "against the air to create lift and drag. A kite consists of wings, tethers, and anchors.\n",
    "Kites often have a bridle to guide the face of the kite at the correct angle so the wind\n",
    "can lift it. A kite’s wing also may be so designed so a bridle is not needed; when\n",
    "kiting a sailplane for launch, the tether meets the wing at a single point. A kite may\n",
    "have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of\n",
    "tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is\n",
    "still often called the kite.\n",
    "The lift that sustains the kite in flight is generated when air flows around the kite’s\n",
    "surface, producing low pressure above and high pressure below the wings. The\n",
    "interaction with the wind also generates horizontal drag along the direction of the\n",
    "wind. The resultant force vector from the lift and drag force components is opposed\n",
    "by the tension of one or more of the lines or tethers to which the kite is attached. The\n",
    "anchor point of the kite line may be static or moving (such as the towing of a kite by\n",
    "a running person, boat, free-falling anchors as in paragliders and fugitive parakites\n",
    "or vehicle).\n",
    "The same principles of fluid flow apply in liquids and kites are also used under water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite\n",
    "lifting surface is called a kytoon.\n",
    "Kites have a long and varied history and many different types are flown\n",
    "individually and at festivals worldwide. Kites may be flown for recreation, art or\n",
    "other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a\n",
    "competition. Power kites are multi-line steerable kites designed to generate large forces\n",
    "which can be used to power activities such as kite surfing, kite landboarding, kite\n",
    "fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have\n",
    "been made.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 16,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         '’': 2,\n",
       "         's': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'such': 2,\n",
       "         'as': 6,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(wikipedia_kite.lower())\n",
    "token_counts = Counter(tokens)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们remove 所有的stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 16,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         '’': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)\n",
    "kite_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以单看token 的计数，我们对文章的主题也是有一些理解的。例如，kite, wing, lift 这些词都很重要，都和kite 相关。如果corpus 中有很多有关kite 的文章，大概率的这些文章string, wind 这些单词的TF 都比较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Vectorizing\n",
    "\n",
    "下面，我们对每个document 生成一个vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc length:  220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07272727272727272,\n",
       " 0.06363636363636363,\n",
       " 0.03636363636363636,\n",
       " 0.022727272727272728,\n",
       " 0.01818181818181818,\n",
       " 0.01818181818181818,\n",
       " 0.013636363636363636,\n",
       " 0.013636363636363636,\n",
       " 0.013636363636363636,\n",
       " 0.00909090909090909]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "print('doc length: ', doc_length)\n",
    "\n",
    "for key, value in kite_counts.most_common(10):\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要对两个document 生成两个vector，然后进行数学操作，例如比较。我们需要这两个vector 在相同的空间内，即有相同的dimension。所以我们需要进行以下操作：\n",
    "- normalize vector\n",
    "- 每个vector 有相同的dimension(构建lexicon)\n",
    "\n",
    "下面，我们用一个简单的例子，一个corpus 有3个documents，我们根据3个documents 构建一个lexicon(相当于vocabulary)，这个lexicon 的长度就是我们document vector 的纬度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The faster Harry got to the store, the faster and faster Harry would get home.', 'Harry is hairy and faster than Jill.', 'Jill is not as hairy as Harry.']\n"
     ]
    }
   ],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of tokens of the 1th document: 17\n",
      "the number of tokens of the 2th document: 8\n",
      "the number of tokens of the 3th document: 8\n"
     ]
    }
   ],
   "source": [
    "doc_tokens = []\n",
    "\n",
    "i = 0\n",
    "for doc in docs:\n",
    "    i += 1\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "    print('the number of tokens of the {}th document: {}'.format(i, len(doc_tokens[i-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))  # 去重\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以这个corpus 中每个document 的vector 的长度为18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    # 复制一个初始化全零的vector\n",
    "    vec = copy.copy(zero_vector)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)  # normalization\n",
    "        \n",
    "    doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.16666666666666666),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vector space\n",
    "\n",
    "A space is the collection of all possible vectors.\n",
    "\n",
    "**rectilinear**\n",
    "\n",
    "#### distance:\n",
    "- Euclidean distance (2-norm distance)\n",
    "    - bad idea for word count vectors (因为每个document 的长度不一样，所以导致很相似的两个文章的距离会很远)\n",
    "- Cosine similarity\n",
    "    - $v_1 . v_2 = |v_1||v_2|cos \\theta$\n",
    "    - cosine similarity has a convenient range for most machine learning problems: -1 to +1.\n",
    "    - 1: identical normalized vectors that point in exactly **the same direction** along all dimensions.\n",
    "        - 如果两个document vectors 的cos 距离接近1: two documents are using similar words in similar proportion - talking about the same thing\n",
    "    - 0: share no components - orthogonal, perpendicular in all dimensions.\n",
    "        - 两个documents 没有一个词是共有的：talking about completely different things.\n",
    "    - -1: compeletely opposite: \n",
    "        - 在word count vector 的场景中不可能出现，因为count 不可能为负。所以所有的word count vector 都在相同的quadrant象限.\n",
    "\n",
    "⚠️只是word count vector不可能出现cos distance 为负的情况，但在其他场景下有可能。我们后面会介绍vectors that more accurately model a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\" Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Zipf's Law\n",
    "\n",
    "Given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. The first item in the ranked list will appear twice as often as the second, and three times as often as the third. \n",
    "\n",
    "所以通过log-log plot 出来应该是一条直线。\n",
    "\n",
    "Zipf’s Law applies to counts of lots of things.\n",
    "\n",
    "为了验证Zipf's law, 我们首先下载一个corpus，然后对其token 进行统计，然后看Zipf's law 是否可行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/chenwang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()[:10]  # returns the tokenized corpus as a sequence of strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())  # 一共有1161192 个tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(brown.words()))  # 一共有56057 个distinct tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[:5]  # Part of Speech Taggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens:  1013821\n",
      "number of unique tokens 49801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "puncs = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "print('number of tokens: ', len(list(word_list)))\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "print('number of unique tokens', len(set(list(word_list))))\n",
    "\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "token_counts = Counter(word_list)\n",
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，去除标点之后，token 个数减少了。如果全部小写之后，unique token的个数变少了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_20_brown = token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "x = list(range(20))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69971, 36412, 28853, 26158, 23195, 21337, 10594, 10109, 9815, 9548, 9489, 8760, 7289, 7253, 6996, 6741, 6377, 5372, 5306, 5164]\n"
     ]
    }
   ],
   "source": [
    "y = list([most_common_20_brown[x_i][1] for x_i in x])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.09446948982055, 15.15212636534252, 14.816433710902283, 14.674964618382953, 14.5015262257372, 14.381069725844188, 13.370959793297384, 13.303352669926333, 13.260772552353533, 13.220982851081777, 13.212040341238552, 13.096715154488537, 12.831505185085327, 12.824362133569485, 12.772314573921653, 12.718746909901062, 12.638662165859655, 12.391243589427443, 12.373408960228696, 12.33427328530719]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "y_log = list([math.log(y_i, 2) for y_i in y])\n",
    "print(y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a18ffafd0>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 280x210 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w = 4\n",
    "h = 3\n",
    "d = 70\n",
    "plt.figure(figsize=(w, h), dpi=d)\n",
    "plt.loglog(x[:10], y[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看出，基本是一条直线。for a sufficiently large sample, the first word in that ranked list is twice as likely to occur in the corpus as the second word in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling\n",
    "\n",
    "我们现在有一个count vector，或者normalized by the length of the document, 都只能告诉我们每个词在当前document 中是否重要(assumption: 使用频率高的词更重要)。但是我们不知道这些词在当前document 相对其他document 是否更重要。例如，有些词可能在所有documents 中的频率都很高，这在某些场景中，这并没有提供什么信息(it doesn’t help distinguish between those\n",
    "documents)。\n",
    "\n",
    "**Inverse document frequency**, or **IDF**, is your window through Zipf in topic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_history = \"\"\"Kites were invented in China, where materials ideal for kite building were readily\n",
    "available: silk fabric for sail material; fine, high-tensile-strength silk for flying line;\n",
    "and resilient bamboo for a strong, lightweight framework.\n",
    "The kite has been claimed as the invention of the 5th-century BC Chinese\n",
    "philosophers Mozi (also Mo Di) and Lu Ban (also Gongshu Ban). By 549 AD\n",
    "paper kites were certainly being flown, as it was recorded that in that year a paper\n",
    "kite was used as a message for a rescue mission. Ancient and medieval Chinese\n",
    "sources describe kites being used for measuring distances, testing the wind, lifting\n",
    "men, signaling, and communication for military operations. The earliest known\n",
    "Chinese kites were flat (not bowed) and often rectangular. Later, tailless kites\n",
    "incorporated a stabilizing bowline. Kites were decorated with mythological motifs\n",
    "and legendary figures; some were fitted with strings and whistles to make musical\n",
    "sounds while flying. From China, kites were introduced to Cambodia, Thailand,\n",
    "India, Japan, Korea and the western world.\n",
    "After its introduction into India, the kite further evolved into the fighter kite, known\n",
    "as the patang in India, where thousands are flown every year on festivals such as\n",
    "Makar Sankranti.\n",
    "Kites were known throughout Polynesia, as far as New Zealand, with the\n",
    "assumption being that the knowledge diffused from China along with the people.\n",
    "Anthropomorphic kites made from cloth and wood were used in religious ceremonies\n",
    "to send prayers to the gods. Polynesian kite traditions are used by anthropologists get\n",
    "an idea of early “primitive” Asian traditions that are believed to have at one time\n",
    "existed in Asia.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens in introduction document:  365\n",
      "total tokens in history document:  297\n"
     ]
    }
   ],
   "source": [
    "kite_intro = wikipedia_kite.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "\n",
    "kite_history = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(kite_history)\n",
    "\n",
    "intro_total = len(intro_tokens)\n",
    "history_total = len(history_tokens)\n",
    "\n",
    "print('total tokens in introduction document: ', intro_total)\n",
    "print('total tokens in history document: ', history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看一下term frequency of “kite” in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"kite\" in intro is: 0.0438\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency of \"kite\" in intro is: {:.4f}'.format(intro_tf['kite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"kite\" in history is: 0.0202\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency of \"kite\" in history is: {:.4f}'.format(history_tf['kite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in intro is: 0.0274\n",
      "Term Frequency of \"and\" in history is: 0.0303\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "\n",
    "print('Term Frequency of \"and\" in intro is: {:.4f}'.format(intro_tf['and']))\n",
    "print('Term Frequency of \"and\" in history is: {:.4f}'.format(history_tf['and']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算IDF\n",
    "A term’s IDF is merely the ratio of the total number of documents to the number of documents the term appears in.\n",
    "\n",
    "- IDF of and: 2 total documents / 2 documents contain “and” = 2/2 = 1\n",
    "- IDF of kite: 2 total documents / 2 documents contain “kite” = 2/2 = 1\n",
    "- IDF of China: 2 total documents / 1 document contains “China” = 2/1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs that contain and:  2\n"
     ]
    }
   ],
   "source": [
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "        \n",
    "print('number of docs that contain and: ', num_docs_containing_and)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs that contain china:  1\n",
      "number of docs that contain kite:  2\n"
     ]
    }
   ],
   "source": [
    "num_docs_containing_kite = 0 \n",
    "num_docs_containing_china = 0\n",
    "\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1\n",
    "        \n",
    "print('number of docs that contain china: ', num_docs_containing_china)\n",
    "print('number of docs that contain kite: ', num_docs_containing_kite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['china'] = intro_counts['china'] / intro_total\n",
    "history_tf['china'] = history_counts['china'] / history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}\n",
    "\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and\n",
    "history_idf['and'] = num_docs / num_docs_containing_and\n",
    "\n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "\n",
    "intro_idf['china'] = num_docs / num_docs_containing_china\n",
    "history_idf['china'] = num_docs / num_docs_containing_china"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算TF-IDF\n",
    "\n",
    "每个document，对lexicon 中的每个词，又一个TF-IDF值。\n",
    "所以，每个document 有一个TF-IDF 向量，长度为lexicon 中token 的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0.0273972602739726, 'kite': 0.043835616438356165, 'china': 0.0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0.030303030303030304,\n",
       " 'kite': 0.020202020202020204,\n",
       " 'china': 0.020202020202020204}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipf's Law\n",
    "\n",
    "Zipf’s Law showed that when you compare the frequencies of two words, even if they occur a similar number of times, the more frequent word will have an exponentially higher frequency than the less frequent one.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "tf-idf assigns a numeric value to the importance of that **word (t)** in the given **document (d)**, given its usage across the entire **corpus (D)**.\n",
    "\n",
    "for a given term, t, in a given document, d, in a corpus, D, you get:\n",
    "- $tf(t, d) = \\frac{count(t)}{count(d)}$ \n",
    "- $idf(t, D) = log \\frac{number of documents}{number of documents containing t}$ \n",
    "- $tf_idf(t, d, D) = tf(t, d) * idf(t, D)$\n",
    "\n",
    "有时，为方便计算，我们可以对上面三个公式左右都取log：\n",
    "- $log_tf(t, d) = log(count(t)) - log(count(d))$\n",
    "- $log_tf(t, d) = log(log(number of documents) - log(number of documents containing t))$\n",
    "- $log_tf_idf(t, d, D) = log(tf(t, d)) + log(idf(t, D))$\n",
    "\n",
    "So,\n",
    "- 一个词在一个document 中出现的越多, TF (and hence the TF-IDF) 越高. \n",
    "- 同时，这个单词在许多document 中都出现了，IDF (and hence the TF-IDF)越低."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Relevance Ranking\n",
    "\n",
    "下面，我们计算每个document 的TF-IDF vector, 这个vector 可以更加全面的reflect meaning (or topic) of the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tfidf_vectors = []\n",
    "\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "\n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个document 是一个 K-dimensional vector，在上面的例子里，K=18.然后我们可以根据vector 的cos distance来计算两个documents 的相似度。\n",
    "\n",
    "下面，我们可以做一个简单的基于TF-IDF 的search. The objective is to find the documents whose vectors have the highest cosine similarities to the query and return those as the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'how': 1,\n",
       "         'long': 1,\n",
       "         'does': 1,\n",
       "         'it': 1,\n",
       "         'take': 1,\n",
       "         'to': 2,\n",
       "         'get': 1,\n",
       "         'the': 1,\n",
       "         'store': 1,\n",
       "         '?': 1})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    \n",
    "    for _doc in docs:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key += 1\n",
    "        \n",
    "    # dropped the keys that weren’t found in the lexicon to avoid a divide-by-zero error\n",
    "    if docs_containing_key == 0:\n",
    "        continue\n",
    "            \n",
    "    tf = value / len(tokens)\n",
    "    idf = len(docs) / docs_containing_key\n",
    "    query_vec[key] = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6132857433407973"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果可以看出，document 0 has the most relevance for your query.\n",
    "\n",
    "上面，我们对没有见过的token 直接丢掉，为了防止divide-by-zero 问题。更好的方法是使用add-one smoothing (laplace smoothing). 对每个被除数+1. 通过使用这个方法，通常能提高keyword based 搜索的准确率。\n",
    "\n",
    "#### chatbot based on keyword-based search engine:\n",
    "- store your training data in pairs of questions (or statements) and appropriate responses. \n",
    "- use TF-IDF to search for a question (or statement) most like the user input text.\n",
    "- return the response associated with that statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
