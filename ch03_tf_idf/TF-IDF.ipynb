{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch03 TF-IDF\n",
    "\n",
    "**Question to answer:** Which words are more important to a particular document and across the corpus as a whole?\n",
    "\n",
    "If you have an idea about the frequency with which those words appear in a document in relation to the rest of the documents, you can use that to further refine the “positivity” of the document.\n",
    "\n",
    "之前，每个word vector 都是一个binary vector, 每一位都是0/1,表示absence / presence。下一步，用continuous number。\n",
    "\n",
    "TF-IDF: term frequency times inverse document frequency\n",
    "- Term Frequency: the number of times a word occurs in a given document\n",
    "    - 有时需要normalize: divided by the number of tokens in the document\n",
    "    - normalize 反映了 relative importance to the document of that term\n",
    "- Inverse Document Frequency: divide each of those word counts by the number of documents in which the word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of Words\n",
    "\n",
    "Bag of word 是一种把document 转化成vector 的方法。\n",
    "- bag of words (BOW) -- word:count pair\n",
    "- binary bag of words (set of words) -- word:bool pair (1:presence/0:absence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "harry_sample_sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "harry_sample_tokens = tokenizer.tokenize(harry_sample_sentence.lower())\n",
    "print(harry_sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: get unique words and word counts\n",
    "\n",
    "我们使用Counter，是一个unordered collection, 也称为bag or multiset. 我们可以将一个list 传给Counter 来生成一个multiset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "harry_sample_bag_of_words = Counter(harry_sample_tokens)\n",
    "harry_sample_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_sample_bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"the\"和\",\"没有什么information，所以可以忽略。我们通过剩余两个most common的tokens，基本可以看出这句话的意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: 计算每个token 的TF\n",
    "\n",
    "下面，我们计算harry 的TF. \n",
    "\n",
    "⚠️这里为什么是除以unique words 的个数，而不是document 的单词个数？这样除有可能大于1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: 2\n",
      "number of words: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears = harry_sample_bag_of_words['harry']  # harry 计数\n",
    "print('counter: {}'.format(times_harry_appears))\n",
    "\n",
    "num_unique_words = len(harry_sample_bag_of_words)  # \n",
    "print('number of words: {}'.format(num_unique_words))\n",
    "\n",
    "tf = times_harry_appears / num_unique_words\n",
    "round(tf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real example\n",
    "\n",
    "下面我们来看一个real example. 我们选取了wikipedia 上有关kite 介绍的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_kite = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react\n",
    "against the air to create lift and drag. A kite consists of wings, tethers, and anchors.\n",
    "Kites often have a bridle to guide the face of the kite at the correct angle so the wind\n",
    "can lift it. A kite’s wing also may be so designed so a bridle is not needed; when\n",
    "kiting a sailplane for launch, the tether meets the wing at a single point. A kite may\n",
    "have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of\n",
    "tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is\n",
    "still often called the kite.\n",
    "The lift that sustains the kite in flight is generated when air flows around the kite’s\n",
    "surface, producing low pressure above and high pressure below the wings. The\n",
    "interaction with the wind also generates horizontal drag along the direction of the\n",
    "wind. The resultant force vector from the lift and drag force components is opposed\n",
    "by the tension of one or more of the lines or tethers to which the kite is attached. The\n",
    "anchor point of the kite line may be static or moving (such as the towing of a kite by\n",
    "a running person, boat, free-falling anchors as in paragliders and fugitive parakites\n",
    "or vehicle).\n",
    "The same principles of fluid flow apply in liquids and kites are also used under water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite\n",
    "lifting surface is called a kytoon.\n",
    "Kites have a long and varied history and many different types are flown\n",
    "individually and at festivals worldwide. Kites may be flown for recreation, art or\n",
    "other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a\n",
    "competition. Power kites are multi-line steerable kites designed to generate large forces\n",
    "which can be used to power activities such as kite surfing, kite landboarding, kite\n",
    "fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have\n",
    "been made.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique tokens in kite introduction text:  180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 16,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         '’': 2,\n",
       "         's': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'such': 2,\n",
       "         'as': 6,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# case normalization\n",
    "# tokenization\n",
    "kite_intro_tokens = tokenizer.tokenize(wikipedia_kite.lower())\n",
    "\n",
    "# multiset: bag of words\n",
    "kite_intro_bag_of_words = Counter(kite_intro_tokens)\n",
    "\n",
    "print('the number of unique tokens in kite introduction text: ', len(kite_intro_bag_of_words))\n",
    "kite_intro_bag_of_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们remove 所有的stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique tokens in kite introduction text (without stop words):  146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 16,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         '’': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# get list of stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# remove stop words\n",
    "kite_intro_tokens_wo_stopwords = [x for x in kite_intro_tokens if x not in stopwords]\n",
    "\n",
    "kite_intro_bag_of_words_wo_stopwords = Counter(kite_intro_tokens_wo_stopwords)\n",
    "\n",
    "print('the number of unique tokens in kite introduction text (without stop words): ', len(kite_intro_bag_of_words_wo_stopwords))\n",
    "\n",
    "kite_intro_bag_of_words_wo_stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以单看token 的计数，我们对文章的主题也是有一些理解的。例如，kite, wing, lift 这些词都很重要，都和kite 相关。如果corpus 中有很多有关kite 的文章，大概率的这些文章string, wind 这些单词的TF 都比较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vectorizing\n",
    "\n",
    "前面，我们计算了每个document 中每个token 的count。下面，我们对每个document 生成一个vector. vector 的长度为字典的大小。通常，对于一个document 的意思，我们通常取频率最高的K个单词即可，下面我们选取最常用的10个词来生成一个**计数占比**的vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc length:  220\n",
      "doc tokens:  ['kite', 'traditionally', 'tethered', 'heavier-than-air', 'craft', 'wing', 'surfaces', 'react', 'air', 'create']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07272727272727272,\n",
       " 0.06363636363636363,\n",
       " 0.03636363636363636,\n",
       " 0.022727272727272728,\n",
       " 0.01818181818181818,\n",
       " 0.01818181818181818,\n",
       " 0.013636363636363636,\n",
       " 0.013636363636363636,\n",
       " 0.013636363636363636,\n",
       " 0.00909090909090909]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "\n",
    "doc_length = len(kite_intro_tokens_wo_stopwords)\n",
    "print('doc length: ', doc_length)\n",
    "print('doc tokens: ', kite_intro_tokens_wo_stopwords[:10])\n",
    "\n",
    "for key, value in kite_intro_bag_of_words_wo_stopwords.most_common(10):\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 每个document 计算一个vector\n",
    "\n",
    "如果需要对两个document 生成两个vector，然后进行数学操作，例如比较。我们需要这两个vector 在相同的空间内，即有相同的dimension。所以我们需要进行以下操作：\n",
    "- normalize vector\n",
    "- 每个vector 有相同的dimension(构建lexicon)\n",
    "\n",
    "下面，我们用一个简单的例子，一个corpus 有3个documents，我们根据3个documents 构建一个lexicon(相当于vocabulary)，这个lexicon 的长度就是我们document vector 的纬度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: 构建corpus，包含三个documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The faster Harry got to the store, the faster and faster Harry would get home.', 'Harry is hairy and faster than Jill.', 'Jill is not as hairy as Harry.']\n"
     ]
    }
   ],
   "source": [
    "sample_docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "sample_docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "sample_docs.append(\"Jill is not as hairy as Harry.\")\n",
    "\n",
    "print(sample_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: 对每个document 进行tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of tokens of the 1th document: 17\n",
      "the number of tokens of the 2th document: 8\n",
      "the number of tokens of the 3th document: 8\n"
     ]
    }
   ],
   "source": [
    "sample_docs_tokens = []\n",
    "\n",
    "i = 0\n",
    "for doc in sample_docs:\n",
    "    i += 1\n",
    "    sample_docs_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "    print('the number of tokens of the {}th document: {}'.format(i, len(sample_docs_tokens[i-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs_all_tokens = sum(sample_docs_tokens, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_docs_all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: 构建lexicon: unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_docs_lexicon = sorted(set(sample_docs_all_tokens))  # 去重\n",
    "len(sample_docs_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_docs_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以这个corpus 中每个document 的vector 的长度为18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: 对每个document，构建长度等于lexicon 的TF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# 首先构建一个初始化全为零的计数器\n",
    "zero_vector = OrderedDict((token, 0) for token in sample_docs_lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "sample_docs_vectors = []\n",
    "\n",
    "# 对每个document 生成一个vector，append 到sample_docs_vectors 列表\n",
    "for doc in sample_docs:\n",
    "    \n",
    "    # 复制一个初始化全零的vector\n",
    "    vec = copy.copy(zero_vector)\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(sample_docs_lexicon)  # ⚠️normalization -- 分母是lexicon 的大小\n",
    "        \n",
    "    sample_docs_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.16666666666666666),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_docs_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，每个document 可以使用一个word count vector 来表示。下面我们先介绍以下vector space, 以及相似度定义。然后我们介绍一个有关计数的通用法则：Zipf's law.\n",
    "\n",
    "### 2.2 Vector space\n",
    "\n",
    "A space is the collection of all possible vectors.\n",
    "\n",
    "**rectilinear**\n",
    "\n",
    "#### distance:\n",
    "- Euclidean distance (2-norm distance)\n",
    "    - bad idea for word count vectors (因为每个document 的长度不一样，所以导致很相似的两个文章的距离会很远)\n",
    "- Cosine similarity\n",
    "    - $v_1 . v_2 = |v_1||v_2|cos \\theta$\n",
    "    - cosine similarity has a convenient range for most machine learning problems: -1 to +1.\n",
    "    - 1: identical normalized vectors that point in exactly **the same direction** along all dimensions.\n",
    "        - 如果两个document vectors 的cos 距离接近1: two documents are using similar words in similar proportion - talking about the same thing\n",
    "    - 0: share no components - orthogonal, perpendicular in all dimensions.\n",
    "        - 两个documents 没有一个词是共有的：talking about completely different things.\n",
    "    - -1: compeletely opposite: \n",
    "        - 在word count vector 的场景中不可能出现，因为count 不可能为负。所以所有的word count vector 都在相同的quadrant象限.\n",
    "\n",
    "⚠️只是word count vector不可能出现cos distance 为负的情况，但在其他场景下有可能。我们后面会介绍vectors that more accurately model a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\" Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Zipf's Law\n",
    "\n",
    "Given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. The first item in the ranked list will appear twice as often as the second, and three times as often as the third. \n",
    "\n",
    "所以通过log-log plot 出来应该是一条直线。\n",
    "\n",
    "Zipf’s Law applies to counts of lots of things.\n",
    "\n",
    "为了验证Zipf's law, 我们首先下载一个corpus，然后对其token 进行统计，然后看Zipf's law 是否可行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Brown Corpus from NLTK\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 数据集已经做好了tokenization\n",
    "brown.words()[:10]  # returns the tokenized corpus as a sequence of strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())  # 一共有1161192 个tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(brown.words()))  # 一共有56057 个distinct tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[:10]  # Part of Speech Taggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "puncs = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "\n",
    "# word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "# print('number of tokens: ', len(list(word_list)))\n",
    "\n",
    "# word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "# print('number of unique tokens', len(set(list(word_list))))\n",
    "\n",
    "# remove punctuation\n",
    "brown_word_list = (x.lower() for x in brown.words() if x not in puncs)\n",
    "\n",
    "brown_bag_of_words = Counter(brown_word_list)\n",
    "most_common_20_brown = brown_bag_of_words.most_common(20)\n",
    "most_common_20_brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "x = list(range(1, 21))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69971, 36412, 28853, 26158, 23195, 21337, 10594, 10109, 9815, 9548, 9489, 8760, 7289, 7253, 6996, 6741, 6377, 5372, 5306, 5164]\n"
     ]
    }
   ],
   "source": [
    "y = list([most_common_20_brown[x_i - 1][1] for x_i in x])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.09446948982055, 15.15212636534252, 14.816433710902283, 14.674964618382953, 14.5015262257372, 14.381069725844188, 13.370959793297384, 13.303352669926333, 13.260772552353533, 13.220982851081777, 13.212040341238552, 13.096715154488537, 12.831505185085327, 12.824362133569485, 12.772314573921653, 12.718746909901062, 12.638662165859655, 12.391243589427443, 12.373408960228696, 12.33427328530719]\n",
      "[0.0, 1.0, 1.5849625007211563, 2.0, 2.321928094887362, 2.584962500721156, 2.807354922057604, 3.0, 3.1699250014423126, 3.3219280948873626, 3.4594316186372978, 3.5849625007211565, 3.700439718141092, 3.8073549220576037, 3.9068905956085187, 4.0, 4.08746284125034, 4.169925001442312, 4.247927513443585, 4.321928094887363]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "log_y = list([math.log(y_i, 2) for y_i in y])\n",
    "print(y_log)\n",
    "\n",
    "log_x = list([math.log(x_i, 2) for x_i in x])\n",
    "print(log_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "\n",
    "w = 12\n",
    "h = 9\n",
    "d = 40\n",
    "plt.figure(figsize=(w, h), dpi=d)\n",
    "plt.scatter(log_x, log_y, c=\"g\", alpha=0.5, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w = 12\n",
    "h = 9\n",
    "d = 40\n",
    "plt.figure(figsize=(w, h), dpi=d)\n",
    "plt.loglog(x, y, c=\"g\", alpha=0.5, marker='o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看出，基本是一条直线。for a sufficiently large sample, the first word in that ranked list is twice as likely to occur in the corpus as the second word in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling\n",
    "\n",
    "#### bag of word 的缺陷\n",
    "\n",
    "我们现在对每个document 计算一个count vector，或者normalized by the length of the document, 都只能告诉我们每个词在当前document 中是否重要(assumption: 使用频率高的词更重要)。但是我们不知道这些词在当前document 相对其他document 是否更重要。\n",
    "\n",
    "例如，有些词可能在所有documents 中的频率都很高，这在某些场景中，这并没有提供什么信息(it doesn’t help distinguish between those\n",
    "documents)。\n",
    "\n",
    "**Inverse document frequency**, or **IDF**, is your window through Zipf in topic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_history = \"\"\"Kites were invented in China, where materials ideal for kite building were readily\n",
    "available: silk fabric for sail material; fine, high-tensile-strength silk for flying line;\n",
    "and resilient bamboo for a strong, lightweight framework.\n",
    "The kite has been claimed as the invention of the 5th-century BC Chinese\n",
    "philosophers Mozi (also Mo Di) and Lu Ban (also Gongshu Ban). By 549 AD\n",
    "paper kites were certainly being flown, as it was recorded that in that year a paper\n",
    "kite was used as a message for a rescue mission. Ancient and medieval Chinese\n",
    "sources describe kites being used for measuring distances, testing the wind, lifting\n",
    "men, signaling, and communication for military operations. The earliest known\n",
    "Chinese kites were flat (not bowed) and often rectangular. Later, tailless kites\n",
    "incorporated a stabilizing bowline. Kites were decorated with mythological motifs\n",
    "and legendary figures; some were fitted with strings and whistles to make musical\n",
    "sounds while flying. From China, kites were introduced to Cambodia, Thailand,\n",
    "India, Japan, Korea and the western world.\n",
    "After its introduction into India, the kite further evolved into the fighter kite, known\n",
    "as the patang in India, where thousands are flown every year on festivals such as\n",
    "Makar Sankranti.\n",
    "Kites were known throughout Polynesia, as far as New Zealand, with the\n",
    "assumption being that the knowledge diffused from China along with the people.\n",
    "Anthropomorphic kites made from cloth and wood were used in religious ceremonies\n",
    "to send prayers to the gods. Polynesian kite traditions are used by anthropologists get\n",
    "an idea of early “primitive” Asian traditions that are believed to have at one time\n",
    "existed in Asia.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens in introduction document:  365\n",
      "total tokens in history document:  297\n"
     ]
    }
   ],
   "source": [
    "kite_intro = wikipedia_kite.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "intro_doc_length = len(intro_tokens)\n",
    "\n",
    "kite_history = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(kite_history)\n",
    "history_doc_length = len(history_tokens)\n",
    "\n",
    "print('total tokens in introduction document: ', intro_doc_length)\n",
    "print('total tokens in history document: ', history_doc_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算TF\n",
    "\n",
    "下面我们看一下term frequency of “kite” in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_doc_length\n",
    "\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / history_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"kite\" in intro is: 0.0438\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency of \"kite\" in intro is: {:.4f}'.format(intro_tf['kite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"kite\" in history is: 0.0202\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency of \"kite\" in history is: {:.4f}'.format(history_tf['kite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in intro is: 0.0274\n",
      "Term Frequency of \"and\" in history is: 0.0303\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_doc_length\n",
    "history_tf['and'] = history_counts['and'] / history_doc_length\n",
    "\n",
    "print('Term Frequency of \"and\" in intro is: {:.4f}'.format(intro_tf['and']))\n",
    "print('Term Frequency of \"and\" in history is: {:.4f}'.format(history_tf['and']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算IDF\n",
    "A term’s IDF is merely the ratio of the total number of documents to the number of documents the term appears in.\n",
    "\n",
    "- IDF of and: 2 total documents / 2 documents contain “and” = 2/2 = 1\n",
    "- IDF of kite: 2 total documents / 2 documents contain “kite” = 2/2 = 1\n",
    "- IDF of China: 2 total documents / 1 document contains “China” = 2/1 = 2\n",
    "\n",
    "首先，计算and 的idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs that contain and:  2\n"
     ]
    }
   ],
   "source": [
    "num_docs_containing_and = 0\n",
    "\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "        \n",
    "print('number of docs that contain and: ', num_docs_containing_and)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，计算kite 和china 的IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs that contain china:  1\n",
      "number of docs that contain kite:  2\n"
     ]
    }
   ],
   "source": [
    "num_docs_containing_kite = 0 \n",
    "num_docs_containing_china = 0\n",
    "\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1\n",
    "        \n",
    "print('number of docs that contain china: ', num_docs_containing_china)\n",
    "print('number of docs that contain kite: ', num_docs_containing_kite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['china'] = intro_counts['china'] / intro_doc_length\n",
    "history_tf['china'] = history_counts['china'] / history_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}\n",
    "\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and\n",
    "history_idf['and'] = num_docs / num_docs_containing_and\n",
    "\n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "\n",
    "intro_idf['china'] = num_docs / num_docs_containing_china\n",
    "history_idf['china'] = num_docs / num_docs_containing_china"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算TF-IDF\n",
    "\n",
    "每个document，对lexicon 中的每个词，又一个TF-IDF值。\n",
    "所以，每个document 有一个TF-IDF 向量，长度为lexicon 中token 的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0.0273972602739726, 'kite': 0.043835616438356165, 'china': 0.0}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0.030303030303030304,\n",
       " 'kite': 0.020202020202020204,\n",
       " 'china': 0.020202020202020204}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipf's Law\n",
    "\n",
    "Zipf’s Law showed that when you compare the frequencies of two words, even if they occur a similar number of times, the more frequent word will have an exponentially higher frequency than the less frequent one.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "tf-idf assigns a numeric value to the importance of that **word (t)** in the given **document (d)**, given its usage across the entire **corpus (D)**.\n",
    "\n",
    "for a given term, t, in a given document, d, in a corpus, D, you get:\n",
    "- $tf(t, d) = \\frac{count(t)}{count(d)}$ \n",
    "- $idf(t, D) = log \\frac{number of documents}{number of documents containing t}$ \n",
    "- $tf_idf(t, d, D) = tf(t, d) * idf(t, D)$\n",
    "\n",
    "有时，为方便计算，我们可以对上面三个公式左右都取log：\n",
    "- $log_tf(t, d) = log(count(t)) - log(count(d))$\n",
    "- $log_tf(t, d) = log(log(number of documents) - log(number of documents containing t))$\n",
    "- $log_tf_idf(t, d, D) = log(tf(t, d)) + log(idf(t, D))$\n",
    "\n",
    "So,\n",
    "- 一个词在一个document 中出现的越多, TF (and hence the TF-IDF) 越高. \n",
    "- 同时，这个单词在许多document 中都出现了，IDF (and hence the TF-IDF)越低."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Relevance Ranking\n",
    "\n",
    "下面，我们计算每个document 的TF-IDF vector, 这个vector 可以更加全面的reflect meaning (or topic) of the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_tfidf_vectors = []\n",
    "\n",
    "for doc in sample_docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "\n",
    "        for _doc in sample_docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(sample_docs_lexicon)\n",
    "\n",
    "        if docs_containing_key:\n",
    "            idf = len(sample_docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "\n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.5),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个document 是一个 K-dimensional vector，在上面的例子里，K=18.然后我们可以根据vector 的cos distance来计算两个documents 的相似度。\n",
    "\n",
    "下面，我们可以做一个简单的基于TF-IDF 的search. The objective is to find the documents whose vectors have the highest cosine similarities to the query and return those as the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'how': 1,\n",
       "         'long': 1,\n",
       "         'does': 1,\n",
       "         'it': 1,\n",
       "         'take': 1,\n",
       "         'to': 2,\n",
       "         'get': 1,\n",
       "         'the': 1,\n",
       "         'store': 1,\n",
       "         '?': 1})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    \n",
    "    for _doc in sample_docs:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key += 1\n",
    "        \n",
    "    # dropped the keys that weren’t found in the lexicon to avoid a divide-by-zero error\n",
    "    if docs_containing_key == 0:\n",
    "        continue\n",
    "            \n",
    "    tf = value / len(tokens)\n",
    "    idf = len(sample_docs) / docs_containing_key\n",
    "    query_vec[key] = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6132857433407973"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果可以看出，document 0 has the most relevance for your query.\n",
    "\n",
    "上面，我们对没有见过的token 直接丢掉，为了防止divide-by-zero 问题。更好的方法是使用add-one smoothing (laplace smoothing). 对每个被除数+1. 通过使用这个方法，通常能提高keyword based 搜索的准确率。\n",
    "\n",
    "#### chatbot based on keyword-based search engine:\n",
    "- store your training data in pairs of questions (or statements) and appropriate responses. \n",
    "- use TF-IDF to search for a question (or statement) most like the user input text.\n",
    "- return the response associated with that statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tools\n",
    "\n",
    "之前我们都是用自己写的函数来计算TF-IDF，后面我们用python 库来计算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = docs\n",
    "print(corpus)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TFIDFVectorizer` model 会生成一个sparse numpy matrix, 因为许多documents 实际上只用了词汇表(lexicon)中很少的一部分词。\n",
    "The `.todense()` method converts a sparse matrix back into a regular numpy matrix (filling in the\n",
    "gaps with zeros) for your **viewing pleasure**.\n",
    "\n",
    "生成一个TF-IDF matrices，每一行是一个document，每一列是一个token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives\n",
    "\n",
    "to improve the relevance of search, 有很多研究工作在于如何优化IDF part.\n",
    "\n",
    "Search engines (information retrieval systems) match keywords (terms) between queries and documents in a corpus. If you’re building a search engine and want to provide documents that are likely to match what your users are looking for, you should spend some time investigating the alternatives.\n",
    "\n",
    "#### Okapi BM25\n",
    "\n",
    "- ignore duplicate terms (tf 最高为1)\n",
    "- choosing the weighting scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "本节我们介绍了keyword based search。下一章，我们介绍semantic search engine. \n",
    "\n",
    "semantic search 的一个问题是：Semantic word and topic vectors don’t scale to billions of documents. 当你的corpus 太大的时候，例如google，bing等web search engine，更多使用keyword search.\n",
    "\n",
    "TF-IDF vector 是我们pipeline 的第一步，下一步我们介绍，如何通过TF-IDF vector 计算topic vector。\n",
    "\n",
    "本章我们介绍了：\n",
    "- Any web-scale search engine with millisecond response times has the power of a TF-IDF term document matrix hidden under the hood.\n",
    "- Term frequencies must be weighted by their inverse document frequency to ensure the most important, most meaningful words are given the heft they deserve.\n",
    "- Zipf’s law can help you predict the frequencies of all sorts of things, including words, characters, and people.\n",
    "- The rows of a TF-IDF term document matrix can be used as a vector representation of the meanings of those individual words to create a vector space model of word semantics.\n",
    "- Euclidean distance and similarity between pairs of high dimensional vectors doesn’t adequately represent their similarity for most NLP applications.\n",
    "- Cosine distance, the amount of “overlap” between vectors, can be calculated efficiently by just multiplying the elements of normalized vectors together and summing up those products.\n",
    "- Cosine distance is the go-to similarity score for most natural language vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
