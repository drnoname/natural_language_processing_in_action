{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example\n",
    "\n",
    "In this chapter, we are going to show a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. build corpus\n",
    "\n",
    "我们首先创建一个有三个documents 的corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Football is popular in Brasil',\n",
    "         'Basketball is popular in USA',\n",
    "         'Ping-Pong is popular in China']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. preprocessing\n",
    "\n",
    "下面我们对每个document 进行预处理\n",
    "\n",
    "### 2.1 case normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['football is popular in brasil',\n",
       " 'basketball is popular in usa',\n",
       " 'ping-pong is popular in china']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lower = [doc.lower() for doc in corpus]\n",
    "list(corpus_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brasil', 'football', 'in', 'is', 'popular'],\n",
       " ['basketball', 'in', 'is', 'popular', 'usa'],\n",
       " ['china', 'in', 'is', 'ping-pong', 'popular']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "corpus_tokens = []\n",
    "\n",
    "for doc in corpus_lower:\n",
    "    corpus_tokens += [sorted(tokenizer.tokenize(doc))]\n",
    "\n",
    "corpus_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brasil', 'football', 'popular'],\n",
       " ['basketball', 'popular', 'usa'],\n",
       " ['china', 'ping-pong', 'popular']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokens_wo_stop_word = []\n",
    "\n",
    "for doc_tokens in corpus_tokens:\n",
    "    corpus_tokens_wo_stop_word.append([w for w in doc_tokens if w not in stopwords])\n",
    "\n",
    "corpus_tokens_wo_stop_word   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 build lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brasil',\n",
       " 'football',\n",
       " 'popular',\n",
       " 'basketball',\n",
       " 'popular',\n",
       " 'usa',\n",
       " 'china',\n",
       " 'ping-pong',\n",
       " 'popular']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_all_tokens = sum(corpus_tokens_wo_stop_word, [])\n",
    "corpus_all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basketball', 'brasil', 'china', 'football', 'ping-pong', 'popular', 'usa']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lexicon = sorted(set(corpus_all_tokens))  # 去重\n",
    "corpus_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('basketball', 0),\n",
       "             ('brasil', 0),\n",
       "             ('china', 0),\n",
       "             ('football', 0),\n",
       "             ('ping-pong', 0),\n",
       "             ('popular', 0),\n",
       "             ('usa', 0)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_vector = OrderedDict((token, 0) for token in corpus_lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('basketball', 0),\n",
       "              ('brasil', 1),\n",
       "              ('china', 0),\n",
       "              ('football', 1),\n",
       "              ('ping-pong', 0),\n",
       "              ('popular', 1),\n",
       "              ('usa', 0)]),\n",
       " OrderedDict([('basketball', 1),\n",
       "              ('brasil', 0),\n",
       "              ('china', 0),\n",
       "              ('football', 0),\n",
       "              ('ping-pong', 0),\n",
       "              ('popular', 1),\n",
       "              ('usa', 1)]),\n",
       " OrderedDict([('basketball', 0),\n",
       "              ('brasil', 0),\n",
       "              ('china', 1),\n",
       "              ('football', 0),\n",
       "              ('ping-pong', 1),\n",
       "              ('popular', 1),\n",
       "              ('usa', 0)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_vectors = []\n",
    "\n",
    "# 对每个document 生成一个vector，append 到sample_docs_vectors 列表\n",
    "for doc_tokens in corpus_tokens_wo_stop_word:\n",
    "    \n",
    "    # 复制一个初始化全零的vector\n",
    "    vec = copy.copy(zero_vector)\n",
    "    \n",
    "    # tokenization\n",
    "    token_counts = Counter(doc_tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value\n",
    "        \n",
    "    corpus_vectors.append(vec)\n",
    "    \n",
    "corpus_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，bag of words model 最后出来的是下面一个matrix\n",
    "\n",
    "|    BOW     | basketball | brasil | china | football | ping-pong | popular | usa |\n",
    "|------------|------------|--------|-------|----------|-----------|---------|-----|\n",
    "| document 1 |      0     |    1   |   0   |     1    |     0     |    1    |  0  |\n",
    "| document 2 |      1     |    0   |   0   |     0    |     0     |    1    |  1  |\n",
    "| docuemnt 3 |      0     |    0   |   1   |     0    |     1     |    1    |  0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF\n",
    "\n",
    "下面，我们介绍TF-IDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Term Frequency (TF)\n",
    "\n",
    "#### 计算公式\n",
    "for a given term, **t**, in a given document, **d**, in a corpus, **D**, you get:\n",
    "\n",
    "$tf(t, d) = \\frac{count(t)}{count(d)}$\n",
    "\n",
    "每个token 的计数除以该document 中所有token 的个数。\n",
    "\n",
    "根据上述公式，我们可以得出以下计算结果：\n",
    "\n",
    "|    TF      | basketball | brasil | china | football | ping-pong | popular | usa |\n",
    "|------------|------------|--------|-------|----------|-----------|---------|-----|\n",
    "| document 1 |      0     |   1/3  |   0   |    1/3   |     0     |   1/3   |  0  |\n",
    "| document 2 |     1/3    |    0   |   0   |     0    |     0     |   1/3   | 1/3 |\n",
    "| docuemnt 3 |      0     |    0   |  1/3  |     0    |    1/3    |   1/3   |  0  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('basketball', 0),\n",
       "              ('brasil', 0.3333333333333333),\n",
       "              ('china', 0),\n",
       "              ('football', 0.3333333333333333),\n",
       "              ('ping-pong', 0),\n",
       "              ('popular', 0.3333333333333333),\n",
       "              ('usa', 0)]),\n",
       " OrderedDict([('basketball', 0.3333333333333333),\n",
       "              ('brasil', 0),\n",
       "              ('china', 0),\n",
       "              ('football', 0),\n",
       "              ('ping-pong', 0),\n",
       "              ('popular', 0.3333333333333333),\n",
       "              ('usa', 0.3333333333333333)]),\n",
       " OrderedDict([('basketball', 0),\n",
       "              ('brasil', 0),\n",
       "              ('china', 0.3333333333333333),\n",
       "              ('football', 0),\n",
       "              ('ping-pong', 0.3333333333333333),\n",
       "              ('popular', 0.3333333333333333),\n",
       "              ('usa', 0)])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tf = []\n",
    "\n",
    "for doc_tokens in corpus_tokens_wo_stop_word:\n",
    "    \n",
    "    doc_length = len(doc_tokens)\n",
    "    \n",
    "    # 复制一个初始化全零的vector\n",
    "    vec = copy.copy(zero_vector)\n",
    "    \n",
    "    # tokenization\n",
    "    token_counts = Counter(doc_tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / doc_length\n",
    "        \n",
    "    corpus_tf.append(vec)\n",
    "    \n",
    "corpus_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inverse Document Frequency (IDF)\n",
    "\n",
    "#### 计算公式\n",
    "\n",
    "for a given term, **t**, in a given document, **d**, in a corpus, **D**, you get:\n",
    "\n",
    "$idf(t, D) = log \\frac{number\\:of\\:documents}{number\\:of\\:documents\\:containing\\:t}$ \n",
    "\n",
    "所有document 的个数除以包含某个token 的documents 的个数。\n",
    "\n",
    "根据上述公式，我们可以得出以下计算结果：\n",
    "\n",
    "\n",
    "| token      | IDF           |\n",
    "|------------|---------------|\n",
    "| basketball | log(3/1) = log3 = 0.477 |\n",
    "| brasil     | log(3/1) = log3 = 0.477 |\n",
    "| china      | log(3/1) = log3 = 0.477 |\n",
    "| football   | log(3/1) = log3 = 0.477 |\n",
    "| ping-pong  | log(3/1) = log3 = 0.477 |\n",
    "| popular    | log(3/3) = 0     |\n",
    "| usa        | log(3/1) = log3 = 0.477 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_idf = {}\n",
    "total_document = len(corpus)\n",
    "\n",
    "for token in corpus_lexicon:\n",
    "    count = 0\n",
    "    for doc_tokens in corpus_tokens_wo_stop_word:\n",
    "        if token in doc_tokens:\n",
    "            count += 1\n",
    "    \n",
    "    corpus_idf[token] = log(total_document / count, 10)  # 注意，这里是log\n",
    "    # corpus_idf[token] = log(((1 + total_document) / (1 + count)), 10) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basketball': 0.47712125471966244,\n",
       " 'brasil': 0.47712125471966244,\n",
       " 'china': 0.47712125471966244,\n",
       " 'football': 0.47712125471966244,\n",
       " 'ping-pong': 0.47712125471966244,\n",
       " 'popular': 0.0,\n",
       " 'usa': 0.47712125471966244}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 TF-IDF\n",
    "\n",
    "$tf\\_idf(t, d, D) = tf(t, d) * idf(t, D)$\n",
    "\n",
    "根据上述公式，我们可以得出以下计算结果：\n",
    "\n",
    "\n",
    "|    TF      | basketball | brasil  | china | football | ping-pong | popular | usa |\n",
    "|------------|------------|---------|-------|----------|-----------|---------|-----|\n",
    "| document 1 |      0     | 1/3*log3|   0   | 1/3*log3 |     0     |   1/3*0 |  0  |\n",
    "| document 2 |  1/3*log3  |    0    |   0   |     0    |     0     |   1/3*0 | 1/3*log3 |\n",
    "| docuemnt 3 |      0     |    0    |1/3*log3|     0   |  1/3*log3 |   1/3*0 |  0  |\n",
    "\n",
    "#### 最终结果\n",
    "\n",
    "|    TF      | basketball | brasil | china | football | ping-pong | popular | usa |\n",
    "|------------|------------|--------|-------|----------|-----------|---------|-----|\n",
    "| document 1 |      0     |   0.16 |   0   |    0.16  |     0     |    0    |  0  |\n",
    "| document 2 |      0.16  |    0   |   0   |     0    |     0     |    0    | 0.16|\n",
    "| docuemnt 3 |      0     |    0   |  0.16 |     0    |    0.16   |    0    |  0  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('basketball', 0.0),\n",
       "              ('brasil', 0.15904041823988746),\n",
       "              ('china', 0.0),\n",
       "              ('football', 0.15904041823988746),\n",
       "              ('ping-pong', 0.0),\n",
       "              ('popular', 0.0),\n",
       "              ('usa', 0.0)]),\n",
       " OrderedDict([('basketball', 0.15904041823988746),\n",
       "              ('brasil', 0.0),\n",
       "              ('china', 0.0),\n",
       "              ('football', 0.0),\n",
       "              ('ping-pong', 0.0),\n",
       "              ('popular', 0.0),\n",
       "              ('usa', 0.15904041823988746)]),\n",
       " OrderedDict([('basketball', 0.0),\n",
       "              ('brasil', 0.0),\n",
       "              ('china', 0.15904041823988746),\n",
       "              ('football', 0.0),\n",
       "              ('ping-pong', 0.15904041823988746),\n",
       "              ('popular', 0.0),\n",
       "              ('usa', 0.0)])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = []\n",
    "\n",
    "for tf in corpus_tf:\n",
    "    \n",
    "    vec = copy.copy(zero_vector)\n",
    "    \n",
    "    for key, value in tf.items():\n",
    "        vec[key] = value * corpus_idf[key]\n",
    "    \n",
    "    tf_idf.append(vec)\n",
    "\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 使用sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.65 0.   0.65 0.   0.39 0.  ]\n",
      " [0.65 0.   0.   0.   0.   0.39 0.65]\n",
      " [0.   0.   0.65 0.   0.65 0.39 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# corpus_wo_stopwords = []\n",
    "\n",
    "# for doc_tokens in corpus_tokens_wo_stop_word:\n",
    "#     corpus_wo_stopwords.append(' '.join(doc_tokens))\n",
    "\n",
    "# print('corpus without stopwords: ', corpus_wo_stopwords)\n",
    "\n",
    "\"\"\"\n",
    "使用与前面相同的预处理\n",
    "- tokenizer\n",
    "- lowercase\n",
    "- stopwords\n",
    "\"\"\" \n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize, stop_words='english', lowercase=True)\n",
    "\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basketball', 'brasil', 'china', 'football', 'ping-pong', 'popular', 'usa']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
